{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266c2c62-33fe-49f7-b45f-ac866546ce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES 输出目录: /root/Invertebrates_EC50_multi_fusion/SMILES/smiles_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# ========== 随机种子 ==========\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ========== 路径配置（按你之前 notebook）==========\n",
    "DATA_PATH   = \"/root/fusion_dataset/Invertebrates_EC50_unique.xlsx\"  # 原始数据\n",
    "MODEL_DIR   = \"/root/多模态/model\"                          # 本地 ChemBERTa 模型目录\n",
    "SMILES_OUT_DIR = Path(\"/root/Invertebrates_EC50_multi_fusion/SMILES/smiles_outputs\")\n",
    "SMILES_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 统一嵌入输出路径（全体样本 CLS）\n",
    "EMB_ALL_PATH = SMILES_OUT_DIR / \"reg_smiles_cls_embeddings_all.npy\"\n",
    "\n",
    "print(\"SMILES 输出目录:\", SMILES_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a407c6-e8c5-4e95-b7a6-8ed18779590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 这里的 MODEL_DIR 要跟你保存 ChemBERTa 模型的路径一致\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb092468-3b84-47d8-87e0-789347b43745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34d0354-62f7-47cc-a7c5-89cf3a214d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理后的 df 前 5 行：\n",
      "     SMILES_Canonical_RDKit  Duration_Value(hour) Effect Endpoint  mgperL  \\\n",
      "0        [Cl-].[Cl-].[Zn+2]                  96.0    ITX     EC50     1.3   \n",
      "1  O=S(=O)([O-])[O-].[Zn+2]                  24.0    ITX     EC50     2.5   \n",
      "2        [Cl-].[Cl-].[Pb+2]                  96.0    ITX     EC50    40.8   \n",
      "3  O=S(=O)([O-])[O-].[Cu+2]                  24.0    ITX     EC50     1.9   \n",
      "4  O=S(=O)([O-])[O-].[Cu+2]                  96.0    ITX     EC50     0.6   \n",
      "\n",
      "   Species Group                         ChemicalName        CAS  \\\n",
      "0  Invertebrates                Zinc chloride (ZnCl2)  7646-85-7   \n",
      "1  Invertebrates       Sulfuric acid, Zinc salt (1:1)  7733-02-0   \n",
      "2  Invertebrates                Lead chloride (PbCl2)  7758-95-4   \n",
      "3  Invertebrates  Sulfuric acid copper(2+) salt (1:1)  7758-98-7   \n",
      "4  Invertebrates  Sulfuric acid copper(2+) salt (1:1)  7758-98-7   \n",
      "\n",
      "            CanonicalSMILES database  mgperL_log  row_id  \n",
      "0        [Cl-].[Cl-].[Zn+2]   ECOTOX    0.113943       0  \n",
      "1  O=S(=O)([O-])[O-].[Zn+2]   ECOTOX    0.397940       1  \n",
      "2        [Cl-].[Cl-].[Pb+2]   ECOTOX    1.610660       2  \n",
      "3  O=S(=O)([O-])[O-].[Cu+2]   ECOTOX    0.278754       3  \n",
      "4  O=S(=O)([O-])[O-].[Cu+2]   ECOTOX   -0.221849       4  \n",
      "样本总数: 3620\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "\n",
    "# 2. 必要列检查\n",
    "required_cols = [\n",
    "    \"SMILES_Canonical_RDKit\",\n",
    "    \"mgperL\",\n",
    "]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"原始表缺少必要列: {missing}\")\n",
    "\n",
    "# 3. 计算 log10 毒性（过滤掉 mgperL <= 0 的异常）\n",
    "df = df.copy()\n",
    "df[\"mgperL_log\"] = df[\"mgperL\"].apply(lambda x: math.log10(x) if x is not None and x > 0 else np.nan)\n",
    "df = df.dropna(subset=[\"mgperL_log\"]).reset_index(drop=True)\n",
    "\n",
    "# 4. row_id（方便将来对齐，如果需要）\n",
    "df[\"row_id\"] = df.index\n",
    "\n",
    "print(\"预处理后的 df 前 5 行：\")\n",
    "print(df.head())\n",
    "print(\"样本总数:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17a6e9-ca60-41ad-aedd-01d4484315cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231a4939-0f6f-4e1c-9d99-c156307c9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 普通 Dataset：用于提取嵌入（预编码 tokenizer，推理快）=====\n",
    "class SMILESDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles, targets, tokenizer, max_length=512):\n",
    "        self.smiles = list(smiles)\n",
    "        self.targets = np.array(targets, dtype=np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encodings = self.tokenizer(\n",
    "            self.smiles,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encodings[\"input_ids\"][idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "\n",
    "# ===== 含随机 SMILES 的增强版 Dataset：仅用于 BERT 训练 =====\n",
    "class SMILESDatasetAug(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    - augment=True: 每次 __getitem__ 时随机生成一个 random SMILES（如果 mol 可以转）\n",
    "    - augment=False: 使用 canonical SMILES\n",
    "    \"\"\"\n",
    "    def __init__(self, smiles, targets, tokenizer, max_length=512, augment=False):\n",
    "        self.smiles = list(smiles)   # canonical SMILES\n",
    "        self.targets = np.array(targets, dtype=np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "        # 预先转成 RDKit Mol\n",
    "        self.mols = []\n",
    "        for s in self.smiles:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(s)\n",
    "            except Exception:\n",
    "                mol = None\n",
    "            self.mols.append(mol)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) 生成一个 SMILES 序列\n",
    "        mol = self.mols[idx]\n",
    "        if self.augment and (mol is not None):\n",
    "            # 随机 SMILES\n",
    "            smiles_str = Chem.MolToSmiles(mol, doRandom=True)\n",
    "        else:\n",
    "            smiles_str = self.smiles[idx]\n",
    "\n",
    "        # 2) tokenizer 编码\n",
    "        encoding = self.tokenizer(\n",
    "            smiles_str,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        input_ids = torch.tensor(encoding[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(encoding[\"attention_mask\"], dtype=torch.long)\n",
    "        label = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9989d-09fb-4c85-8d2a-6637a190cd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee7f65c-acf2-45f0-b5c7-b74b92936efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 文本端模型定义：ChemBERTa 回归器 + CLS 嵌入提取接口（768 维）\n",
    "\n",
    "class ChemBERTaRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    ChemBERTa 回归模型：\n",
    "    - backbone: 预训练 Roberta/ChemBERTa\n",
    "    - head: 基于 [CLS] 向量的一层 Linear 回归头\n",
    "    - get_cls_embedding: 返回 CLS 作为下游 RF / 融合的输入特征\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path,\n",
    "        freeze_embeddings: bool = True,\n",
    "        freeze_n_layers: int = 0,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = RobertaModel.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reg_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # 冻结 embedding 层\n",
    "        if freeze_embeddings:\n",
    "            for p in self.backbone.embeddings.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 冻结前 freeze_n_layers 个 encoder layer\n",
    "        if freeze_n_layers > 0:\n",
    "            encoder_layers = self.backbone.encoder.layer\n",
    "            for layer in encoder_layers[:freeze_n_layers]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        cls = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "        x = self.dropout(cls)\n",
    "        x = self.reg_head(x).squeeze(-1)          # (B,)\n",
    "        return x\n",
    "\n",
    "    def get_cls_embedding(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.backbone(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            cls = outputs.last_hidden_state[:, 0, :]\n",
    "        return cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c37fcb-92e9-4c44-a661-633257499c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092a0642-639a-4e24-a469-730634b54b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 训练集样本数: 2889\n",
      "BERT 验证/测试集样本数: 731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](|[Cl])(|[Cl])CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](|[Cl])(|[Cl])CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](|[Cl])(|[Cl])CCCC' for input: '[Cl]|[Sn](|[Cl])(|[Cl])CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCCCCCC[S]|[Sn](|[S]CCCCCCCCCCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 16:\n",
      "[00:26:15] CCCCCCCCCCCC[S]|[Sn](|[S]CCCCCCCCCCCC)(CC\n",
      "[00:26:15] ~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCCCCCC[S]|[Sn](|[S]CCCCCCCCCCCC)(CCCC)CCCC' for input: 'CCCCCCCCCCCC[S]|[Sn](|[S]CCCCCCCCCCCC)(CCCC)CCCC'\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [K+].[K+].[K+].[Fe-3](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 23:\n",
      "[00:26:15] +].[K+].[K+].[Fe-3](|[C]#N)(|[C]#N)(|[C]#\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[K+].[K+].[K+].[Fe-3](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N' for input: '[K+].[K+].[K+].[Fe-3](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 13:\n",
      "[00:26:15] CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC\n",
      "[00:26:15] ~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC' for input: 'CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 13:\n",
      "[00:26:15] CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC\n",
      "[00:26:15] ~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC' for input: 'CCCCCCCC[Sn]|1(|[O]C(=O)C[S]|1)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]C\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]C\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCCCCCC'\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](|[Cl])(CCCCCCCC)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](|[Cl])(CCCCCCCC)CCCCCCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](|[Cl])(CCCCCCCC)CCCCCCCC' for input: '[Cl]|[Sn](|[Cl])(CCCCCCCC)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](|[Cl])(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](|[Cl])(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](|[Cl])(CCCC)CCCC' for input: '[Cl]|[Sn](|[Cl])(CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](|[Cl])(C)C\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](|[Cl])(C)C\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](|[Cl])(C)C' for input: '[Cl]|[Sn](|[Cl])(C)C'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Ni]|1|2(|[O]C(=CC(=[OH]|1)C)C)|[O]C(=CC(=[OH]|2)C)C\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Ni]|1|2(|[O]C(=CC(=[OH]|1)C)C)|[O]C(=CC(\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Ni]|1|2(|[O]C(=CC(=[OH]|1)C)C)|[O]C(=CC(=[OH]|2)C)C' for input: '[Ni]|1|2(|[O]C(=CC(=[OH]|1)C)C)|[O]C(=CC(=[OH]|2)C)C'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Zn](|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO)|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 6:\n",
      "[00:26:15] [Zn](|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H\n",
      "[00:26:15] ~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Zn](|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO)|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO' for input: '[Zn](|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO)|OC(=O)[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Zn++](|[C-]#N)|[C-]#N\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 8:\n",
      "[00:26:15] [Zn++](|[C-]#N)|[C-]#N\n",
      "[00:26:15] ~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Zn++](|[C-]#N)|[C-]#N' for input: '[Zn++](|[C-]#N)|[C-]#N'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Zn++](|[C-]#N)|[C-]#N\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 8:\n",
      "[00:26:15] [Zn++](|[C-]#N)|[C-]#N\n",
      "[00:26:15] ~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Zn++](|[C-]#N)|[C-]#N' for input: '[Zn++](|[C-]#N)|[C-]#N'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Sn](CCCC)(CCCC)CCCC)|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Sn](CCCC)(CCCC)CCCC)|[Sn](CCCC)(CCCC)\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Sn](CCCC)(CCCC)CCCC)|[Sn](CCCC)(CCCC)CCCC' for input: 'O(|[Sn](CCCC)(CCCC)CCCC)|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Na+].[Na+].[Na+].[Cr+3]|1|2|3|4(|[O-]c5ccc6ccccc6c5N=N|1c7c([O-]|2)cc(c8cc(ccc78)[N+]([O-])=O)[S]([O-])(=O)=O)|[O-]c9ccc%10ccccc%10c9N=N|3c%11c([O-]|4)cc(c%12cc(ccc%11%12)[N+]([O-])=O)[S]([O-])(=O)=O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 25:\n",
      "[00:26:15] ].[Na+].[Na+].[Cr+3]|1|2|3|4(|[O-]c5ccc6c\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Na+].[Na+].[Na+].[Cr+3]|1|2|3|4(|[O-]c5ccc6ccccc6c5N=N|1c7c([O-]|2)cc(c8cc(ccc78)[N+]([O-])=O)[S]([O-])(=O)=O)|[O-]c9ccc%10ccccc%10c9N=N|3c%11c([O-]|4)cc(c%12cc(ccc%11%12)[N+]([O-])=O)[S]([O-])(=O)=O' for input: '[Na+].[Na+].[Na+].[Cr+3]|1|2|3|4(|[O-]c5ccc6ccccc6c5N=N|1c7c([O-]|2)cc(c8cc(ccc78)[N+]([O-])=O)[S]([O-])(=O)=O)|[O-]c9ccc%10ccccc%10c9N=N|3c%11c([O-]|4)cc(c%12cc(ccc%11%12)[N+]([O-])=O)[S]([O-])(=O)=O'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Zn]|OC(=O)C(C)=C\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Zn]|OC(=O)C(C)=C\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Zn]|OC(=O)C(C)=C' for input: 'O|[Zn]|OC(=O)C(C)=C'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[In](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[In](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[In](|O)|O' for input: 'O|[In](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[In](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[In](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[In](|O)|O' for input: 'O|[In](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[In](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[In](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[In](|O)|O' for input: 'O|[In](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Na+].[Na+].[Na+].[Na+].[Cu]|1|Oc2cc(ccc2N\\N=C\\3C(=O|1)c4c(N)cc(cc4C=C3[S]([O-])(=O)=O)[S]([O-])(=O)=O)c5ccc6N\\N=C\\7C(=O|[Cu]|Oc6c5)c8c(N)cc(cc8C=C7[S]([O-])(=O)=O)[S]([O-])(=O)=O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 29:\n",
      "[00:26:15] a+].[Na+].[Na+].[Cu]|1|Oc2cc(ccc2N\\N=C\\3C\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Na+].[Na+].[Na+].[Na+].[Cu]|1|Oc2cc(ccc2N\\N=C\\3C(=O|1)c4c(N)cc(cc4C=C3[S]([O-])(=O)=O)[S]([O-])(=O)=O)c5ccc6N\\N=C\\7C(=O|[Cu]|Oc6c5)c8c(N)cc(cc8C=C7[S]([O-])(=O)=O)[S]([O-])(=O)=O' for input: '[Na+].[Na+].[Na+].[Na+].[Cu]|1|Oc2cc(ccc2N\\N=C\\3C(=O|1)c4c(N)cc(cc4C=C3[S]([O-])(=O)=O)[S]([O-])(=O)=O)c5ccc6N\\N=C\\7C(=O|[Cu]|Oc6c5)c8c(N)cc(cc8C=C7[S]([O-])(=O)=O)[S]([O-])(=O)=O'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [OH-]|[Pt](|[OH-])(|[OH-])(|[OH-])(|[OH-])|[OH-].[H+].[H+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 6:\n",
      "[00:26:15] [OH-]|[Pt](|[OH-])(|[OH-])(|[OH-])(|[OH-]\n",
      "[00:26:15] ~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[OH-]|[Pt](|[OH-])(|[OH-])(|[OH-])(|[OH-])|[OH-].[H+].[H+]' for input: '[OH-]|[Pt](|[OH-])(|[OH-])(|[OH-])(|[OH-])|[OH-].[H+].[H+]'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 8:\n",
      "[00:26:15] [Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC\n",
      "[00:26:15] ~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]' for input: '[Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 8:\n",
      "[00:26:15] [Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC\n",
      "[00:26:15] ~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]' for input: '[Ti+4](|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]CC(CC)(COCC=C)COCC=C)(|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC)|[O-]P(OCCCCCCCCCCCCC)OCCCCCCCCCCCCC.[H+].[H+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: N.N.O|[Ti](|O)(|OC(C)C(O)=O)|OC(C)C(O)=O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 6:\n",
      "[00:26:15] N.N.O|[Ti](|O)(|OC(C)C(O)=O)|OC(C)C(O)=O\n",
      "[00:26:15] ~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'N.N.O|[Ti](|O)(|OC(C)C(O)=O)|OC(C)C(O)=O' for input: 'N.N.O|[Ti](|O)(|OC(C)C(O)=O)|OC(C)C(O)=O'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sn](CCCC)(CCCC)CCCC\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sn](CCCC)(CCCC)CCCC' for input: '[Cl]|[Sn](CCCC)(CCCC)CCCC'\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]' for input: '[Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH4+].[NH4+].[Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 19:\n",
      "[00:26:15] [NH4+].[NH4+].[Cl]|[Pd--](|[Cl])(|[Cl])(|\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH4+].[NH4+].[Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[NH4+].[NH4+].[Cl]|[Pd--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Pd]|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Pd]|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Pd]|O' for input: 'O|[Pd]|O'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Co](|O)|O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Co](|O)|O\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Co](|O)|O' for input: 'O|[Co](|O)|O'\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] Explicit valence for atom # 1 O, 4, is greater than permitted\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]' for input: '[Cl]|[Sb](|[Cl])(|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] Explicit valence for atom # 17 O, 2, is greater than permitted\n",
      "[00:26:15] Explicit valence for atom # 0 O, 2, is greater than permitted\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC' for input: 'CCCCCCCC[Sn](|[O]C(=O)C(C)(C)CCC(C)(C)C)(|[O]C(=O)C(C)(C)CCC(C)(C)C)CCCCCCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)|[S]CC(=O)OCC(CC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 14:\n",
      "[00:26:15] CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]C\n",
      "[00:26:15] ~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)|[S]CC(=O)OCC(CC)CCCC' for input: 'CCCCCCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)|[S]CC(=O)OCC(CC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Na+].[Na+].[Na+].[Na+].[Fe-4](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 32:\n",
      "[00:26:15] .[Na+].[Na+].[Fe-4](|[C]#N)(|[C]#N)(|[C]#\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Na+].[Na+].[Na+].[Na+].[Fe-4](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N' for input: '[Na+].[Na+].[Na+].[Na+].[Fe-4](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [K+].[K+].[K+].[K+].[Fe](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 26:\n",
      "[00:26:15] [K+].[K+].[K+].[Fe](|[C]#N)(|[C]#N)(|[C]#\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[K+].[K+].[K+].[K+].[Fe](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N' for input: '[K+].[K+].[K+].[K+].[Fe](|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)(|[C]#N)|[C]#N'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: CCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCC\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 10:\n",
      "[00:26:15] CCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O\n",
      "[00:26:15] ~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'CCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCC' for input: 'CCCC[Sn](|[S]CC(=O)OCC(CC)CCCC)(|[S]CC(=O)OCC(CC)CCCC)CCCC'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 19:\n",
      "[00:26:15] [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]' for input: '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 19:\n",
      "[00:26:15] [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]' for input: '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 19:\n",
      "[00:26:15] [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]' for input: '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 19:\n",
      "[00:26:15] [NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]' for input: '[NH4+].[NH4+].[Cl]|[Zn--](|[Cl])(|[Cl])|[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]' for input: '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]' for input: '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[K+].[K+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]' for input: '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 5:\n",
      "[00:26:15] [Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[\n",
      "[00:26:15] ~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]' for input: '[Cl]|[Pt--](|[Cl])(|[Cl])(|[Cl])(|[Cl])|[Cl].[H+].[H+]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [Na+].[Na+].[Cu]|1|OC(=O)CN(CCN(CC(O|1)=O)CC([O-])=O)CC([O-])=O\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 17:\n",
      "[00:26:15] [Na+].[Na+].[Cu]|1|OC(=O)CN(CCN(CC(O|1)=O\n",
      "[00:26:15] ~~~~~~~~~~~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[Na+].[Na+].[Cu]|1|OC(=O)CN(CCN(CC(O|1)=O)CC([O-])=O)CC([O-])=O' for input: '[Na+].[Na+].[Cu]|1|OC(=O)CN(CCN(CC(O|1)=O)CC([O-])=O)CC([O-])=O'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train steps/epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O|[Hg]c1c([O-])c(Br)cc2c1Oc3cc([O-])c(Br)cc3C24OC(=O)c5ccccc45.[Na+].[Na+]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 2:\n",
      "[00:26:15] O|[Hg]c1c([O-])c(Br)cc2c1Oc3cc([O-])c(Br)\n",
      "[00:26:15] ~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O|[Hg]c1c([O-])c(Br)cc2c1Oc3cc([O-])c(Br)cc3C24OC(=O)c5ccccc45.[Na+].[Na+]' for input: 'O|[Hg]c1c([O-])c(Br)cc2c1Oc3cc([O-])c(Br)cc3C24OC(=O)c5ccccc45.[Na+].[Na+]'\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Ag])|[Ag]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Ag])|[Ag]\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Ag])|[Ag]' for input: 'O(|[Ag])|[Ag]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6' for input: 'O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 3:\n",
      "[00:26:15] O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc\n",
      "[00:26:15] ~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES 'O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6' for input: 'O(|[Sn](CC(C)(C)c1ccccc1)(CC(C)(C)c2ccccc2)CC(C)(C)c3ccccc3)|[Sn](CC(C)(C)c4ccccc4)(CC(C)(C)c5ccccc5)CC(C)(C)c6ccccc6'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 7:\n",
      "[00:26:15] [NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[C\n",
      "[00:26:15] ~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]' for input: '[NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]'\n",
      "[00:26:15] SMILES Parse Error: syntax error while parsing: [NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]\n",
      "[00:26:15] SMILES Parse Error: check for mistakes around position 7:\n",
      "[00:26:15] [NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[C\n",
      "[00:26:15] ~~~~~~^\n",
      "[00:26:15] SMILES Parse Error: Failed parsing SMILES '[NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]' for input: '[NH2-]|[Pd++](|[NH2-])(|[NH2-])|[NH2-].[Cl].[Cl]'\n"
     ]
    }
   ],
   "source": [
    "# ========= 4. 文本端：基于 SMILES 分组做 8:2 划分，仅用于 BERT 训练 =========\n",
    "\n",
    "smiles_all  = df[\"SMILES_Canonical_RDKit\"].astype(str).tolist()\n",
    "targets_all = df[\"mgperL_log\"].values\n",
    "groups_all  = df[\"SMILES_Canonical_RDKit\"].astype(str).values  # 分组依据\n",
    "\n",
    "gss_bert = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx_bert, val_idx_bert = next(gss_bert.split(smiles_all, targets_all, groups=groups_all))\n",
    "\n",
    "smiles_train = [smiles_all[i] for i in train_idx_bert]\n",
    "smiles_val   = [smiles_all[i] for i in val_idx_bert]\n",
    "y_train_bert = targets_all[train_idx_bert]\n",
    "y_val_bert   = targets_all[val_idx_bert]\n",
    "\n",
    "print(\"BERT 训练集样本数:\", len(smiles_train))\n",
    "print(\"BERT 验证/测试集样本数:\", len(smiles_val))\n",
    "\n",
    "# ===== 构建 Dataset / DataLoader =====\n",
    "ENC_BATCH_SIZE = 64\n",
    "MAX_EPOCHS     = 100\n",
    "PATIENCE       = 10\n",
    "MAX_LEN        = 512\n",
    "\n",
    "train_enc_dataset = SMILESDatasetAug(\n",
    "    smiles_train, y_train_bert, tokenizer,\n",
    "    max_length=MAX_LEN, augment=False,  # train 做增强\n",
    ")\n",
    "val_enc_dataset = SMILESDatasetAug(\n",
    "    smiles_val, y_val_bert, tokenizer,\n",
    "    max_length=MAX_LEN, augment=False,  # val 不增强\n",
    ")\n",
    "\n",
    "train_enc_loader = torch.utils.data.DataLoader(\n",
    "    train_enc_dataset, batch_size=ENC_BATCH_SIZE,\n",
    "    shuffle=True, drop_last=False,\n",
    ")\n",
    "val_enc_loader = torch.utils.data.DataLoader(\n",
    "    val_enc_dataset, batch_size=ENC_BATCH_SIZE,\n",
    "    shuffle=False, drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"Train steps/epoch:\", math.ceil(len(train_enc_dataset) / ENC_BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f6559-c14f-4b95-8d3c-73f1f350d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ebccb4b-952b-4ec2-b092-3be6245ddc18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /root/多模态/model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch [1/100] - Training: 100%|██████████| 46/46 [00:29<00:00,  1.56it/s, Reg Loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Reg Loss: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Validation: 100%|██████████| 12/12 [00:02<00:00,  4.53it/s, Reg Loss=0.408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.5038, Val MAE: 0.8710, Val R²: 0.1786, Early Stop Counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] - Training: 100%|██████████| 46/46 [00:35<00:00,  1.28it/s, Reg Loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Reg Loss: 0.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] - Validation: 100%|██████████| 12/12 [00:03<00:00,  3.64it/s, Reg Loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.4883, Val MAE: 0.8510, Val R²: 0.1648, Early Stop Counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] - Training: 100%|██████████| 46/46 [00:44<00:00,  1.04it/s, Reg Loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Reg Loss: 0.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] - Validation: 100%|██████████| 12/12 [00:06<00:00,  2.00it/s, Reg Loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.4681, Val MAE: 0.8264, Val R²: 0.2125, Early Stop Counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] - Training: 100%|██████████| 46/46 [00:38<00:00,  1.19it/s, Reg Loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Reg Loss: 0.3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] - Validation: 100%|██████████| 12/12 [00:03<00:00,  3.25it/s, Reg Loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.5327, Val MAE: 0.9018, Val R²: 0.0598, Early Stop Counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] - Training: 100%|██████████| 46/46 [00:38<00:00,  1.18it/s, Reg Loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Reg Loss: 0.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] - Validation: 100%|██████████| 12/12 [00:03<00:00,  3.53it/s, Reg Loss=0.277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.4778, Val MAE: 0.8294, Val R²: 0.1702, Early Stop Counter: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] - Training: 100%|██████████| 46/46 [00:41<00:00,  1.11it/s, Reg Loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Reg Loss: 0.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] - Validation: 100%|██████████| 12/12 [00:04<00:00,  2.86it/s, Reg Loss=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.4614, Val MAE: 0.8299, Val R²: 0.2746, Early Stop Counter: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] - Training: 100%|██████████| 46/46 [00:39<00:00,  1.18it/s, Reg Loss=0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Reg Loss: 0.3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] - Validation: 100%|██████████| 12/12 [00:02<00:00,  4.68it/s, Reg Loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.5058, Val MAE: 0.8707, Val R²: 0.1418, Early Stop Counter: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] - Training: 100%|██████████| 46/46 [00:49<00:00,  1.08s/it, Reg Loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Reg Loss: 0.5431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] - Validation: 100%|██████████| 12/12 [00:11<00:00,  1.08it/s, Reg Loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.6111, Val MAE: 1.0061, Val R²: -0.0004, Early Stop Counter: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] - Training: 100%|██████████| 46/46 [00:59<00:00,  1.29s/it, Reg Loss=0.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Reg Loss: 0.5626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] - Validation: 100%|██████████| 12/12 [00:10<00:00,  1.18it/s, Reg Loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.6150, Val MAE: 1.0090, Val R²: -0.0958, Early Stop Counter: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] - Training: 100%|██████████| 46/46 [01:06<00:00,  1.45s/it, Reg Loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Reg Loss: 0.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] - Validation: 100%|██████████| 12/12 [00:11<00:00,  1.06it/s, Reg Loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.6014, Val MAE: 0.9939, Val R²: -0.0321, Early Stop Counter: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] - Training: 100%|██████████| 46/46 [00:55<00:00,  1.21s/it, Reg Loss=0.647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Train Reg Loss: 0.5533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] - Validation: 100%|██████████| 12/12 [00:07<00:00,  1.52it/s, Reg Loss=0.959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Reg Loss: 0.6235, Val MAE: 1.0212, Val R²: -0.0074, Early Stop Counter: 4\n",
      "早停触发，在第 11 个 epoch 停止训练\n",
      "\n",
      "✅ BERT 训练完成，best_epoch = 6, best_val_loss = 0.4614\n",
      "✅ 已加载最佳验证损失对应的模型权重，用于后续提取 CLS embeddings。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = train_enc_loader\n",
    "val_loader   = val_enc_loader\n",
    "\n",
    "def compute_metrics_simple(y_true, y_pred):\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return r2, mae\n",
    "\n",
    "# ===== 初始化 encoder =====\n",
    "final_text_model = ChemBERTaRegressor(\n",
    "    MODEL_DIR,\n",
    "    freeze_embeddings=False,\n",
    "    freeze_n_layers=0,   # 你可以调成 6，看你想不想多冻几层\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "# ===== 损失函数 =====\n",
    "reg_criterion = nn.SmoothL1Loss()   # 你原来就是这个，beta 默认 1.0 也可以\n",
    "\n",
    "# ===== 优化器 =====\n",
    "optimizer = AdamW(final_text_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# ===== OneCycleLR 调度器 =====\n",
    "num_epochs = 100\n",
    "patience = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_counter = 0\n",
    "\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=2e-3,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs,\n",
    ")\n",
    "\n",
    "best_state_dict = None\n",
    "best_epoch = -1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --------- 训练 ---------\n",
    "    final_text_model.train()\n",
    "    running_reg_loss = 0.0\n",
    "\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\")\n",
    "    for input_ids, attention_mask, reg_labels in train_loader_tqdm:\n",
    "        input_ids  = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        reg_labels = reg_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # ChemBERTaRegressor 的 forward 是 (input_ids, attention_mask)\n",
    "        reg_output = final_text_model(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)\n",
    "\n",
    "        reg_loss = reg_criterion(reg_output.squeeze(-1), reg_labels)\n",
    "        reg_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_reg_loss += reg_loss.item() * reg_labels.size(0)\n",
    "        train_loader_tqdm.set_postfix({'Reg Loss': reg_loss.item()})\n",
    "\n",
    "    avg_reg_loss = running_reg_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Reg Loss: {avg_reg_loss:.4f}\")\n",
    "\n",
    "    # --------- 验证 ---------\n",
    "    final_text_model.eval()\n",
    "    val_reg_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\")\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, reg_labels in val_loader_tqdm:\n",
    "            input_ids  = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            reg_labels = reg_labels.to(device)\n",
    "\n",
    "            reg_output = final_text_model(input_ids=input_ids,\n",
    "                                          attention_mask=attention_mask)\n",
    "\n",
    "            reg_loss = reg_criterion(reg_output.squeeze(-1), reg_labels)\n",
    "            val_reg_loss += reg_loss.item() * reg_labels.size(0)\n",
    "\n",
    "            all_preds.extend(reg_output.squeeze(-1).cpu().numpy())\n",
    "            all_labels.extend(reg_labels.cpu().numpy())\n",
    "\n",
    "            val_loader_tqdm.set_postfix({'Reg Loss': reg_loss.item()})\n",
    "\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_mae = mean_absolute_error(all_labels, all_preds)\n",
    "    val_r2  = r2_score(all_labels, all_preds)\n",
    "\n",
    "    print(\n",
    "        f\"Val Reg Loss: {avg_val_reg_loss:.4f}, \"\n",
    "        f\"Val MAE: {val_mae:.4f}, \"\n",
    "        f\"Val R²: {val_r2:.4f}, \"\n",
    "        f\"Early Stop Counter: {early_stop_counter}\"\n",
    "    )\n",
    "\n",
    "    # --------- 早停 & 保存最好模型 ---------\n",
    "    # 稍微加一个 1e-4 的裕量，避免浮动抖动\n",
    "    if avg_val_reg_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = avg_val_reg_loss\n",
    "        early_stop_counter = 0\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        # 1) 内存里保存一份，后面直接 load_state_dict 用\n",
    "        best_state_dict = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in final_text_model.state_dict().items()\n",
    "        }\n",
    "        # 2) 硬盘也存一份（你原来的习惯）\n",
    "        torch.save(best_state_dict, SMILES_OUT_DIR / \"best_chemberta_regressor.pth\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"早停触发，在第 {epoch+1} 个 epoch 停止训练\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n✅ BERT 训练完成，best_epoch = {best_epoch}, best_val_loss = {best_val_loss:.4f}\")\n",
    "\n",
    "# 恢复到最佳 checkpoint（如果你后面要用它来抽 embedding）\n",
    "if best_state_dict is not None:\n",
    "    final_text_model.load_state_dict(best_state_dict)\n",
    "    final_text_model.to(device)\n",
    "    final_text_model.eval()\n",
    "    print(\"✅ 已加载最佳验证损失对应的模型权重，用于后续提取 CLS embeddings。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97511e7d-d376-4c53-b9fa-d451fefd1006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe0798d-19fe-4d17-8aff-23914013e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 使用训练好的 encoder 提取全体样本的 CLS 词嵌入 ====\n",
      "CLS 嵌入形状: (3620, 768)\n",
      "✅ All-data CLS embeddings 已保存到: /root/Invertebrates_EC50_multi_fusion/SMILES/smiles_outputs/reg_smiles_cls_embeddings_all.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== 使用训练好的 encoder 提取全体样本的 CLS 词嵌入 ====\")\n",
    "\n",
    "def extract_cls_embeddings(model, smiles, targets, tokenizer, device, batch_size=128, max_length=512):\n",
    "    dataset = SMILESDataset(smiles, targets, tokenizer, max_length=max_length)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    all_embeds = []\n",
    "    with torch.inference_mode():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            input_ids      = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            cls_emb = model.get_cls_embedding(input_ids, attention_mask)  # (B, H)\n",
    "            all_embeds.append(cls_emb.cpu().numpy())\n",
    "\n",
    "    all_embeds = np.concatenate(all_embeds, axis=0)\n",
    "    return all_embeds\n",
    "\n",
    "smiles_all  = df[\"SMILES_Canonical_RDKit\"].astype(str).tolist()\n",
    "targets_all = df[\"mgperL_log\"].values\n",
    "\n",
    "emb_all = extract_cls_embeddings(\n",
    "    final_text_model,\n",
    "    smiles_all,\n",
    "    targets_all,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    batch_size=128,\n",
    "    max_length=MAX_LEN,\n",
    ")\n",
    "\n",
    "print(\"CLS 嵌入形状:\", emb_all.shape)\n",
    "np.save(EMB_ALL_PATH, emb_all)\n",
    "print(f\"✅ All-data CLS embeddings 已保存到: {EMB_ALL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d112c97-fd73-4d8f-8407-4e791cd0fd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448333f7-cfb9-4851-9a07-943e58ba1723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_text_all 形状: (3620, 768)\n",
      "y_all 形状: (3620,)\n",
      "数值型 meta 列: ['Duration_Value(hour)']\n",
      "类别型 meta 列: ['Effect', 'Endpoint']\n",
      "X_meta_train 形状: (2889, 4)\n",
      "X_meta_test  形状: (731, 4)\n",
      "RF 最终 X_train 形状: (2889, 772)\n",
      "RF 最终 X_test  形状: (731, 772)\n",
      "RF 训练集样本数: 2889\n",
      "RF 测试集样本数: 731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# ========= 5. RF 端：独立按 SMILES 分组做 8:2 划分 + 拼接 meta =========\n",
    "\n",
    "# 1) 文本端 CLS 嵌入（刚刚保存过，这里可以直接用 emb_all，也可以从文件读）\n",
    "X_text_all = np.load(EMB_ALL_PATH)        # (N, d_text)\n",
    "y_all      = df[\"mgperL_log\"].values      # (N,)\n",
    "groups_all = df[\"SMILES_Canonical_RDKit\"].astype(str).values\n",
    "\n",
    "print(\"X_text_all 形状:\", X_text_all.shape)\n",
    "print(\"y_all 形状:\", y_all.shape)\n",
    "\n",
    "# 2) 指定你想用的 meta 列\n",
    "#    —— 这里先按你说的 duration / effect / endpoint 来写\n",
    "#    —— 会自动过滤掉 df 里不存在的列，避免 KeyError\n",
    "NUM_META_COLS_CANDIDATE = [\"Duration_Value(hour)\"]          # 数值型\n",
    "CAT_META_COLS_CANDIDATE = [\"Effect\", \"Endpoint\"]      # 类别型\n",
    "\n",
    "NUM_META_COLS = [c for c in NUM_META_COLS_CANDIDATE if c in df.columns]\n",
    "CAT_META_COLS = [c for c in CAT_META_COLS_CANDIDATE if c in df.columns]\n",
    "\n",
    "print(\"数值型 meta 列:\", NUM_META_COLS)\n",
    "print(\"类别型 meta 列:\", CAT_META_COLS)\n",
    "\n",
    "# 3) 先按 SMILES 做 8:2 外层划分（这里 X 用个占位，不用真的特征避免混淆）\n",
    "gss_rf = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=2025)\n",
    "dummy_X = np.zeros((len(df), 1))  # 只为了满足 split 的接口\n",
    "train_idx_rf, test_idx_rf = next(gss_rf.split(dummy_X, y_all, groups=groups_all))\n",
    "\n",
    "y_train = y_all[train_idx_rf]\n",
    "y_test  = y_all[test_idx_rf]\n",
    "groups_train = groups_all[train_idx_rf]\n",
    "\n",
    "# 文本 CLS 部分按索引切\n",
    "X_text_train = X_text_all[train_idx_rf]\n",
    "X_text_test  = X_text_all[test_idx_rf]\n",
    "\n",
    "# 4) 构建 meta 的 DataFrame（先只切原始 df，后面再做编码）\n",
    "df_train_meta = df.iloc[train_idx_rf].copy()\n",
    "df_test_meta  = df.iloc[test_idx_rf].copy()\n",
    "\n",
    "# 5) 数值型 meta：StandardScaler（只在 train 上 fit）\n",
    "if len(NUM_META_COLS) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    X_num_train = scaler.fit_transform(df_train_meta[NUM_META_COLS].values)\n",
    "    X_num_test  = scaler.transform(df_test_meta[NUM_META_COLS].values)\n",
    "else:\n",
    "    # 如果一个数值列也没有，就给个空数组占位\n",
    "    X_num_train = np.zeros((len(df_train_meta), 0), dtype=np.float32)\n",
    "    X_num_test  = np.zeros((len(df_test_meta), 0), dtype=np.float32)\n",
    "\n",
    "# 6) 类别型 meta：OneHotEncoder（同样只在 train 上 fit）\n",
    "if len(CAT_META_COLS) > 0:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_cat_train = ohe.fit_transform(df_train_meta[CAT_META_COLS].astype(str))\n",
    "    X_cat_test  = ohe.transform(df_test_meta[CAT_META_COLS].astype(str))\n",
    "else:\n",
    "    X_cat_train = np.zeros((len(df_train_meta), 0), dtype=np.float32)\n",
    "    X_cat_test  = np.zeros((len(df_test_meta), 0), dtype=np.float32)\n",
    "\n",
    "# 7) 把 meta 数值 + meta one-hot 拼在一起\n",
    "X_meta_train = np.concatenate([X_num_train, X_cat_train], axis=1)\n",
    "X_meta_test  = np.concatenate([X_num_test,  X_cat_test],  axis=1)\n",
    "\n",
    "print(\"X_meta_train 形状:\", X_meta_train.shape)\n",
    "print(\"X_meta_test  形状:\", X_meta_test.shape)\n",
    "\n",
    "# 8) 最终特征 = 文本 CLS + meta\n",
    "X_train = np.concatenate([X_text_train, X_meta_train], axis=1)\n",
    "X_test  = np.concatenate([X_text_test,  X_meta_test],  axis=1)\n",
    "\n",
    "print(\"RF 最终 X_train 形状:\", X_train.shape)\n",
    "print(\"RF 最终 X_test  形状:\", X_test.shape)\n",
    "print(\"RF 训练集样本数:\", len(y_train))\n",
    "print(\"RF 测试集样本数:\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d676a-4aeb-4e3f-9aca-ff1995e5f742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673e5dde-5cfd-4fd1-810d-1090498c2329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 开始在 RF 训练集上做十折随机搜索 (GroupKFold) ====\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=9, n_estimators=900; total time= 2.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=6, n_estimators=299; total time=12.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=543; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=543; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=7, n_estimators=452; total time= 2.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=766; total time= 6.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=366; total time=32.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=3, n_estimators=976; total time=101.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=7, n_estimators=305; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=7, n_estimators=305; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=7, n_estimators=305; total time=  57.2s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=929; total time= 3.6min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=929; total time= 3.8min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=929; total time= 3.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=469; total time= 1.9min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=469; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=9, n_estimators=924; total time= 6.4min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=9, n_estimators=924; total time= 5.8min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=3, min_samples_split=6, n_estimators=991; total time= 3.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=3, min_samples_split=6, n_estimators=991; total time= 3.1min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 2.0min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=720; total time= 4.9min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=262; total time=23.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=689; total time= 3.8min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=8, n_estimators=400; total time= 2.0min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=8, n_estimators=400; total time= 1.8min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=606; total time= 1.6min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=925; total time=39.5min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=966; total time= 4.8min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, n_estimators=774; total time= 3.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, n_estimators=774; total time= 3.6min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, n_estimators=774; total time= 3.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=9, n_estimators=900; total time= 2.7min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=6, n_estimators=299; total time=12.8min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=543; total time= 1.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=391; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=7, n_estimators=452; total time= 2.7min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=1, min_samples_split=4, n_estimators=684; total time=97.4min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=234; total time=16.6min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=234; total time=16.3min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=60.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=874; total time=43.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=35.4min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=9, n_estimators=900; total time= 2.7min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=6, n_estimators=299; total time=12.1min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=6, n_estimators=299; total time=16.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=366; total time=31.5min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=3, n_estimators=976; total time=96.8min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=59.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=874; total time=41.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=35.7min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=2, min_samples_split=4, n_estimators=414; total time=16.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=391; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=7, n_estimators=452; total time= 2.7min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=766; total time= 6.1min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=366; total time=33.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=3, n_estimators=976; total time=99.8min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=58.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=874; total time=41.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=37.0min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=2, min_samples_split=4, n_estimators=414; total time=16.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=543; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=7, n_estimators=452; total time= 2.9min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=1, min_samples_split=4, n_estimators=684; total time=97.1min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 2.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 2.4min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=234; total time=16.8min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=62.3min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=262; total time=23.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=874; total time=39.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=36.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=9, n_estimators=900; total time= 2.6min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=4, min_samples_split=6, n_estimators=299; total time=12.9min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=3, n_estimators=543; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=391; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=766; total time= 5.4min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=1, min_samples_split=4, n_estimators=684; total time=95.7min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 2.1min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=234; total time=16.2min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=63.6min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=262; total time=24.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=874; total time=40.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=35.9min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=9, n_estimators=900; total time= 2.4min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=2, min_samples_split=4, n_estimators=414; total time=18.4min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=1, min_samples_split=4, n_estimators=684; total time=95.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 2.1min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 1.6min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 1.8min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=234; total time=15.5min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=587; total time=62.7min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=720; total time= 4.4min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=262; total time=24.7min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=689; total time= 3.8min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=8, n_estimators=400; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=8, n_estimators=400; total time= 1.8min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=606; total time= 1.9min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=925; total time=36.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=33.5min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=2, min_samples_split=4, n_estimators=414; total time=16.3min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=391; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=7, n_estimators=452; total time= 2.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=766; total time= 5.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=366; total time=30.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=10, n_estimators=366; total time=31.1min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=3, n_estimators=976; total time=93.1min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=9, n_estimators=924; total time= 6.1min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=9, n_estimators=924; total time= 6.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=3, min_samples_split=6, n_estimators=991; total time= 3.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 2.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 2.1min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=8, n_estimators=720; total time= 4.7min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=262; total time=24.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=689; total time= 3.8min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=8, n_estimators=400; total time= 2.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=606; total time= 1.9min\n",
      "[CV] END max_depth=40, max_features=0.5, min_samples_leaf=4, min_samples_split=9, n_estimators=925; total time=37.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=33.8min\n",
      "\n",
      "=== RF 超参搜索完成 ===\n",
      "Best params: {'max_depth': 30, 'max_features': 0.5, 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 234}\n",
      "Best CV R^2 (train 10-fold): 0.5417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ===== 基础 RF 模型 =====\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ===== 超参数搜索空间 =====\n",
    "param_distributions = {\n",
    "    \"n_estimators\":      randint(200, 1001),         # 200 ~ 1000\n",
    "    \"max_depth\":         [None, 10, 20, 30, 40],\n",
    "    \"min_samples_split\": randint(2, 11),             # 2 ~ 10\n",
    "    \"min_samples_leaf\":  randint(1, 5),              # 1 ~ 4\n",
    "    \"max_features\":      [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=10)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,\n",
    "    scoring=\"r2\",\n",
    "    cv=gkf,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"\\n==== 开始在 RF 训练集上做十折随机搜索 (GroupKFold) ====\")\n",
    "rf_search.fit(X_train, y_train, groups=groups_train)\n",
    "\n",
    "best_params   = rf_search.best_params_\n",
    "best_cv_score = rf_search.best_score_\n",
    "\n",
    "print(\"\\n=== RF 超参搜索完成 ===\")\n",
    "print(\"Best params:\", best_params)\n",
    "print(f\"Best CV R^2 (train 10-fold): {best_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795b944-3ca7-444a-b0e6-6ca99b92bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72c2f91-ef11-4181-8151-f1e362a6fc02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 基于最优超参，在同一套 10 折上计算 train OOF 预测 ====\n",
      "  -> OOF fold 1 / 10\n",
      "  -> OOF fold 2 / 10\n",
      "  -> OOF fold 3 / 10\n",
      "  -> OOF fold 4 / 10\n",
      "  -> OOF fold 5 / 10\n",
      "  -> OOF fold 6 / 10\n",
      "  -> OOF fold 7 / 10\n",
      "  -> OOF fold 8 / 10\n",
      "  -> OOF fold 9 / 10\n",
      "  -> OOF fold 10 / 10\n",
      "\n",
      "===== RF 文本端：train OOF 表现（基于 BERT CLS+meta）=====\n",
      "OOF R^2  = 0.5418\n",
      "OOF MAE  = 0.5775\n",
      "OOF RMSE = 0.7993\n",
      "OOF R    = 0.7362\n",
      "\n",
      "===== RF 文本端：训练集表现（基于 BERT CLS+meta）=====\n",
      "Train R^2  = 0.8846\n",
      "Train MAE  = 0.2740\n",
      "Train RMSE = 0.4011\n",
      "Train R    = 0.9452\n",
      "\n",
      "===== RF 文本端：独立测试集表现（基于 BERT CLS+meta）=====\n",
      "Test R^2  = 0.5395\n",
      "Test MAE  = 0.6097\n",
      "Test RMSE = 0.8863\n",
      "Test R    = 0.7388\n",
      "\n",
      "===== RF 文本端：独立测试集表现（基于 BERT CLS 嵌入）=====\n",
      "Test R^2  = 0.5395\n",
      "Test MAE  = 0.6097\n",
      "Test RMSE = 0.8863\n",
      "Test R    = 0.7388\n",
      "\n",
      "✅ RF 相关数据和指标已保存到: /root/Invertebrates_EC50_multi_fusion/SMILES/smiles_outputs\n",
      "   - rf_text_y_train.npy / rf_text_y_test.npy\n",
      "   - rf_text_oof_pred_train.npy  (train OOF，用于 late fusion)\n",
      "   - rf_text_y_pred_test.npy     (test 预测，用于 late fusion)\n",
      "   - rf_text_train_idx.npy / rf_text_test_idx.npy (与 df 行号对应)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# ===== 1) 基于最优超参，在同一套 10 折上计算 train OOF 预测 =====\n",
    "print(\"\\n==== 基于最优超参，在同一套 10 折上计算 train OOF 预测 ====\")\n",
    "\n",
    "# 用和 RandomizedSearch 一样的 GroupKFold 规则重建索引\n",
    "# （GroupKFold 是确定性的，所以这套折和调参时的是同一套）\n",
    "cv_indices_rf = list(gkf.split(X_train, y_train, groups=groups_train))\n",
    "\n",
    "oof_pred_train = np.zeros_like(y_train, dtype=float)\n",
    "\n",
    "for fold_idx, (tr_idx, val_idx) in enumerate(cv_indices_rf, 1):\n",
    "    print(f\"  -> OOF fold {fold_idx} / {len(cv_indices_rf)}\")\n",
    "    rf_fold = RandomForestRegressor(\n",
    "        n_jobs=-1,\n",
    "        random_state=2025 + fold_idx,\n",
    "        **best_params,\n",
    "    )\n",
    "    rf_fold.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "    oof_pred_train[val_idx] = rf_fold.predict(X_train[val_idx])\n",
    "\n",
    "# OOF 指标\n",
    "r2_oof   = r2_score(y_train, oof_pred_train)\n",
    "mae_oof  = mean_absolute_error(y_train, oof_pred_train)\n",
    "mse_oof  = mean_squared_error(y_train, oof_pred_train)\n",
    "rmse_oof = np.sqrt(mse_oof)\n",
    "r_oof, _ = pearsonr(y_train, oof_pred_train)\n",
    "\n",
    "print(\"\\n===== RF 文本端：train OOF 表现（基于 BERT CLS+meta）=====\")\n",
    "print(f\"OOF R^2  = {r2_oof:.4f}\")\n",
    "print(f\"OOF MAE  = {mae_oof:.4f}\")\n",
    "print(f\"OOF RMSE = {rmse_oof:.4f}\")\n",
    "print(f\"OOF R    = {r_oof:.4f}\")\n",
    "\n",
    "# ===== 2) 用最优超参重新在整个训练集上拟合一个 RF =====\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=2025,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# ===== 3) 在训练集和独立测试集上评估 =====\n",
    "y_pred_train = best_rf.predict(X_train)\n",
    "y_pred_test  = best_rf.predict(X_test)\n",
    "\n",
    "# 训练集指标（可选，但顺手算一下）\n",
    "r2_train   = r2_score(y_train, y_pred_train)\n",
    "mae_train  = mean_absolute_error(y_train, y_pred_train)\n",
    "mse_train  = mean_squared_error(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r_train, _ = pearsonr(y_train, y_pred_train)\n",
    "\n",
    "print(\"\\n===== RF 文本端：训练集表现（基于 BERT CLS+meta）=====\")\n",
    "print(f\"Train R^2  = {r2_train:.4f}\")\n",
    "print(f\"Train MAE  = {mae_train:.4f}\")\n",
    "print(f\"Train RMSE = {rmse_train:.4f}\")\n",
    "print(f\"Train R    = {r_train:.4f}\")\n",
    "\n",
    "# 测试集指标（保留你原来的输出）\n",
    "r2_test   = r2_score(y_test, y_pred_test)\n",
    "mae_test  = mean_absolute_error(y_test, y_pred_test)\n",
    "mse_test  = mean_squared_error(y_test, y_pred_test)  # 这里返回的是 MSE\n",
    "rmse_test = np.sqrt(mse_test)                        # 手动转成 RMSE\n",
    "r_test, _ = pearsonr(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n===== RF 文本端：独立测试集表现（基于 BERT CLS+meta）=====\")\n",
    "print(f\"Test R^2  = {r2_test:.4f}\")\n",
    "print(f\"Test MAE  = {mae_test:.4f}\")\n",
    "print(f\"Test RMSE = {rmse_test:.4f}\")\n",
    "print(f\"Test R    = {r_test:.4f}\")\n",
    "\n",
    "print(\"\\n===== RF 文本端：独立测试集表现（基于 BERT CLS 嵌入）=====\")\n",
    "print(f\"Test R^2  = {r2_test:.4f}\")\n",
    "print(f\"Test MAE  = {mae_test:.4f}\")\n",
    "print(f\"Test RMSE = {rmse_test:.4f}\")\n",
    "print(f\"Test R    = {r_test:.4f}\")\n",
    "\n",
    "# ===== 4) 保存一些结果（方便后续融合/画图）=====\n",
    "\n",
    "metrics_rf = {\n",
    "    \"train_metrics\": {\n",
    "        \"r2\":   float(r2_train),\n",
    "        \"mae\":  float(mae_train),\n",
    "        \"rmse\": float(rmse_train),\n",
    "        \"r\":    float(r_train),\n",
    "    },\n",
    "    \"test_metrics\": {   # 原来的不能丢\n",
    "        \"r2\":   float(r2_test),\n",
    "        \"mae\":  float(mae_test),\n",
    "        \"rmse\": float(rmse_test),\n",
    "        \"r\":    float(r_test),\n",
    "    },\n",
    "    \"oof_metrics\": {    # 新增：OOF 指标\n",
    "        \"r2\":   float(r2_oof),\n",
    "        \"mae\":  float(mae_oof),\n",
    "        \"rmse\": float(rmse_oof),\n",
    "        \"r\":    float(r_oof),\n",
    "    },\n",
    "    \"cv_search\": {\n",
    "        \"best_cv_r2\": float(best_cv_score),\n",
    "        \"best_params\": {\n",
    "            k: (int(v) if isinstance(v, np.integer) else v)\n",
    "            for k, v in best_params.items()\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(SMILES_OUT_DIR / \"rf_text_pipeline_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_rf, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 保存特征和标签（保留你原来的）\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_X_train.npy\", X_train.astype(np.float32))\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_y_train.npy\", y_train.astype(np.float32))\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_X_test.npy\",  X_test.astype(np.float32))\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_y_test.npy\",  y_test.astype(np.float32))\n",
    "\n",
    "# 🔹 新增：专门给后期融合读取用的文件\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_oof_pred_train.npy\", oof_pred_train.astype(np.float32))\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_y_pred_test.npy\",    y_pred_test.astype(np.float32))\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_train_idx.npy\",      train_idx_rf.astype(np.int64))  # df 行号\n",
    "np.save(SMILES_OUT_DIR / \"rf_text_test_idx.npy\",       test_idx_rf.astype(np.int64))\n",
    "\n",
    "print(\"\\n✅ RF 相关数据和指标已保存到:\", SMILES_OUT_DIR)\n",
    "print(\"   - rf_text_y_train.npy / rf_text_y_test.npy\")\n",
    "print(\"   - rf_text_oof_pred_train.npy  (train OOF，用于 late fusion)\")\n",
    "print(\"   - rf_text_y_pred_test.npy     (test 预测，用于 late fusion)\")\n",
    "print(\"   - rf_text_train_idx.npy / rf_text_test_idx.npy (与 df 行号对应)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd24d8d-ca3c-41df-9bd7-0aca79f0f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
