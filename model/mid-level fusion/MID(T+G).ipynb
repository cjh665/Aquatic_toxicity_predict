{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773f4e47-f119-468e-9f19-9e2303f8bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: 依赖 & 工具函数 =====\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ---- 全局随机种子 ----\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用设备:\", device)\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"计算 MAE / RMSE / R2 / Pearson_r\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2   = float(r2_score(y_true, y_pred))\n",
    "\n",
    "    if np.std(y_true) == 0 or np.std(y_pred) == 0:\n",
    "        pr = float(\"nan\")\n",
    "    else:\n",
    "        pr, _ = pearsonr(y_true, y_pred)\n",
    "        pr = float(pr)\n",
    "\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"Pearson_r\": pr}\n",
    "\n",
    "\n",
    "def np_encoder(o):\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    raise TypeError(f\"Type {type(o)} not serializable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523fafe-a768-4276-8809-7c07dc54e08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046eb4a0-c686-4013-9895-9b2b8640c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT_EMB_768 : /root/Invertebrates_EC50_multi_fusion/SMILES/smiles_outputs/reg_smiles_cls_embeddings_all.npy\n",
      "GRAPH_EMB_PATH: /root/Invertebrates_EC50_multi_fusion/graph/graph_outputs/reg_graph_embeddings.npy\n",
      "DATA_PATH    : /root/fusion_dataset/Invertebrates_EC50_unique.xlsx\n",
      "OUT_TG       : /root/Invertebrates_EC50_multi_fusion/mid(T+G)/text_graph_meta\n",
      "df 形状: (3620, 12)\n",
      "   row_id    SMILES_Canonical_RDKit  mgperL  mgperL_log\n",
      "0       0        [Cl-].[Cl-].[Zn+2]     1.3    0.113943\n",
      "1       1  O=S(=O)([O-])[O-].[Zn+2]     2.5    0.397940\n",
      "2       2        [Cl-].[Cl-].[Pb+2]    40.8    1.610660\n",
      "3       3  O=S(=O)([O-])[O-].[Cu+2]     1.9    0.278754\n",
      "4       4  O=S(=O)([O-])[O-].[Cu+2]     0.6   -0.221849\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: 路径 & 读 df =====\n",
    "ROOT_MULTI = Path(\"/root/Invertebrates_EC50_multi_fusion\")\n",
    "\n",
    "# Text CLS 全量嵌入（行号对齐 df）\n",
    "TEXT_DIR     = ROOT_MULTI / \"SMILES\" / \"smiles_outputs\"\n",
    "TEXT_EMB_768 = TEXT_DIR / \"reg_smiles_cls_embeddings_all.npy\"\n",
    "\n",
    "# Graph embedding + row_id\n",
    "GRAPH_DIR        = ROOT_MULTI / \"graph\" / \"graph_outputs\"\n",
    "GRAPH_EMB_PATH   = GRAPH_DIR / \"reg_graph_embeddings.npy\"\n",
    "GRAPH_ROWID_PATH = GRAPH_DIR / \"row_id_graph_for_emb.npy\"\n",
    "\n",
    "# 原始数据（含 SMILES / mgperL / Duration / Effect / Endpoint）\n",
    "DATA_PATH = Path(\"/root/fusion_dataset/Invertebrates_EC50_unique.xlsx\")\n",
    "\n",
    "# 输出目录\n",
    "MID_ROOT    = ROOT_MULTI / \"mid(T+G)\"\n",
    "OUT_TG      = MID_ROOT / \"text_graph_meta\"\n",
    "MODELS_CA   = OUT_TG / \"models_crossattn\"\n",
    "MODELS_RF   = OUT_TG / \"models_rf\"\n",
    "for d in [MID_ROOT, OUT_TG, MODELS_CA, MODELS_RF]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 列名\n",
    "SMILES_COL   = \"SMILES_Canonical_RDKit\"\n",
    "DURATION_COL = \"Duration_Value(hour)\"\n",
    "EFFECT_COL   = \"Effect\"\n",
    "ENDPOINT_COL = \"Endpoint\"\n",
    "LABEL_RAW    = \"mgperL\"\n",
    "LABEL_LOG    = \"mgperL_log\"   # 若不存在就自己建\n",
    "LABEL_COL    = LABEL_LOG\n",
    "\n",
    "print(\"TEXT_EMB_768 :\", TEXT_EMB_768)\n",
    "print(\"GRAPH_EMB_PATH:\", GRAPH_EMB_PATH)\n",
    "print(\"DATA_PATH    :\", DATA_PATH)\n",
    "print(\"OUT_TG       :\", OUT_TG)\n",
    "\n",
    "# ===== 读取 df & 构造标签 =====\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "\n",
    "if \"row_id\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "df[\"row_id\"] = df[\"row_id\"].astype(int)\n",
    "\n",
    "if LABEL_LOG not in df.columns:\n",
    "    df[LABEL_RAW] = pd.to_numeric(df[LABEL_RAW], errors=\"coerce\")\n",
    "    mask_valid = df[LABEL_RAW] > 0\n",
    "    df[LABEL_LOG] = np.where(mask_valid, np.log10(df[LABEL_RAW]), np.nan)\n",
    "\n",
    "print(\"df 形状:\", df.shape)\n",
    "print(df[[\"row_id\", SMILES_COL, LABEL_RAW, LABEL_LOG]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5262a-0c47-4df5-b13b-a97ef0bb9d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565abba5-4d9b-43e8-aa42-3027ab9650f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_all_full 形状: (3620, 768)\n",
      "graph_emb 形状: (3213, 256)\n",
      "rowid_graph 范围: 1 → 3619\n",
      "Text+Graph row_id 交集初始样本数: 3213\n",
      "过滤后样本数: 3213\n",
      "X_text  形状: (3213, 768)\n",
      "X_graph 形状: (3213, 256)\n",
      "dur_all_raw 形状: (3213, 1)\n",
      "cat_all     形状: (3213, 3)\n",
      "总样本数 N: 3213\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: 加载 Text & Graph + 对齐 + 构造 meta =====\n",
    "\n",
    "# Text 全量 CLS\n",
    "text_all_full = np.load(TEXT_EMB_768)\n",
    "assert text_all_full.shape[0] == len(df), \"text_all_full 行数应与 df 一致\"\n",
    "print(\"text_all_full 形状:\", text_all_full.shape)\n",
    "\n",
    "# Graph\n",
    "graph_emb   = np.load(GRAPH_EMB_PATH)\n",
    "rowid_graph = np.load(GRAPH_ROWID_PATH).astype(int)\n",
    "print(\"graph_emb 形状:\", graph_emb.shape)\n",
    "print(\"rowid_graph 范围:\", rowid_graph.min(), \"→\", rowid_graph.max())\n",
    "\n",
    "df_indexed = df.set_index(\"row_id\")\n",
    "\n",
    "ids_text  = set(df_indexed.index.tolist())          # text 是按 df 行号\n",
    "ids_graph = set(rowid_graph.tolist())\n",
    "ids_all   = set(df_indexed.index.tolist())\n",
    "\n",
    "ids_inter = sorted(list(ids_text & ids_graph & ids_all))\n",
    "print(\"Text+Graph row_id 交集初始样本数:\", len(ids_inter))\n",
    "\n",
    "idx_map_graph = {rid: i for i, rid in enumerate(rowid_graph)}\n",
    "\n",
    "meta_list   = []\n",
    "X_text_list = []\n",
    "X_graph_list= []\n",
    "y_list      = []\n",
    "rid_list    = []\n",
    "\n",
    "for rid in ids_inter:\n",
    "    row_meta = df_indexed.loc[rid]\n",
    "    label = row_meta[LABEL_COL]\n",
    "    # 要求：label 不缺失，duration/effect/endpoint 都存在\n",
    "    if pd.isna(label) or not np.isfinite(label):\n",
    "        continue\n",
    "    if pd.isna(row_meta.get(DURATION_COL)) or pd.isna(row_meta.get(EFFECT_COL)) or pd.isna(row_meta.get(ENDPOINT_COL)):\n",
    "        continue\n",
    "\n",
    "    meta_list.append(row_meta)\n",
    "    X_text_list.append(text_all_full[rid])            # Text CLS：按 row_id 取\n",
    "    X_graph_list.append(graph_emb[idx_map_graph[rid]])\n",
    "    y_list.append(label)\n",
    "    rid_list.append(rid)\n",
    "\n",
    "meta_tg  = pd.DataFrame(meta_list).reset_index(drop=True)\n",
    "X_text   = np.stack(X_text_list, axis=0)\n",
    "X_graph  = np.stack(X_graph_list, axis=0)\n",
    "y_all    = np.array(y_list, dtype=float)\n",
    "rowid_all= np.array(rid_list, dtype=int)\n",
    "\n",
    "print(\"过滤后样本数:\", len(y_all))\n",
    "print(\"X_text  形状:\", X_text.shape)\n",
    "print(\"X_graph 形状:\", X_graph.shape)\n",
    "\n",
    "# ========== 构造 meta：Duration + Effect/Endpoint one-hot ==========\n",
    "# Duration\n",
    "meta_tg[DURATION_COL] = pd.to_numeric(meta_tg[DURATION_COL], errors=\"coerce\")\n",
    "dur_median = meta_tg[DURATION_COL].median()\n",
    "meta_tg[DURATION_COL] = meta_tg[DURATION_COL].fillna(dur_median)\n",
    "dur_all_raw = meta_tg[[DURATION_COL]].values.astype(float)\n",
    "\n",
    "# One-hot: Effect + Endpoint\n",
    "cat_cols = [EFFECT_COL, ENDPOINT_COL]\n",
    "cat_dummies = pd.get_dummies(meta_tg[cat_cols], dummy_na=False)\n",
    "cat_all = cat_dummies.values.astype(float)\n",
    "cat_feature_names = list(cat_dummies.columns)\n",
    "\n",
    "print(\"dur_all_raw 形状:\", dur_all_raw.shape)\n",
    "print(\"cat_all     形状:\", cat_all.shape)\n",
    "\n",
    "# 分组（按 SMILES）\n",
    "groups_all = meta_tg[SMILES_COL].astype(str).values\n",
    "print(\"总样本数 N:\", len(y_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd65126-70ff-4210-8ae5-ec3194b830d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87581e54-831e-44c9-8257-a225966675ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_text_raw  形状: (3213, 768)\n",
      "X_graph_raw 形状: (3213, 256)\n",
      "dur_raw     形状: (3213, 1)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: 标准化三个数值块（Text / Graph / Duration） =====\n",
    "\n",
    "# Text / Graph / Duration 只用 train 部分来拟合 scaler，但目前我们还没划分。\n",
    "# 这里先对“全部”算一下 raw，后面在 Cross-Attn 和 RF 各自的 8:2 里面再 fit 分别的 scaler 也可以。\n",
    "# 为了简单，Cross-Attn 和 RF 各自有自己的一套 scaler（你说不要求完全一致）。\n",
    "\n",
    "X_text_raw  = X_text        # (N, d_t)\n",
    "X_graph_raw = X_graph       # (N, d_g)\n",
    "dur_raw     = dur_all_raw   # (N, 1)\n",
    "\n",
    "print(\"X_text_raw  形状:\", X_text_raw.shape)\n",
    "print(\"X_graph_raw 形状:\", X_graph_raw.shape)\n",
    "print(\"dur_raw     形状:\", dur_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876233d-f337-42a7-bd0f-52848b52fd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c8581e-5ffd-431f-8af9-e081b698d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5: Dataset & Cross-Attn 模型（Text+Graph） =====\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, X1, X2, y):\n",
    "        self.X1 = torch.from_numpy(X1).float()\n",
    "        self.X2 = torch.from_numpy(X2).float()\n",
    "        self.y  = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class CrossAttnEncoder(nn.Module):\n",
    "    \"\"\"只负责把两模态融合成 fused embedding\"\"\"\n",
    "    def __init__(self, dim_a, dim_b, hidden_dim=256, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj_a = nn.Linear(dim_a, hidden_dim)\n",
    "        self.proj_b = nn.Linear(dim_b, hidden_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, xa, xb):\n",
    "        # xa, xb: (B, d_a/d_b)\n",
    "        h_a = self.proj_a(xa)  # (B, hidden)\n",
    "        h_b = self.proj_b(xb)  # (B, hidden)\n",
    "        tokens = torch.stack([h_a, h_b], dim=0)  # (2, B, hidden)\n",
    "        attn_out, _ = self.attn(tokens, tokens, tokens)  # (2, B, hidden)\n",
    "        fused = attn_out.mean(dim=0)  # (B, hidden)\n",
    "        return self.dropout(fused)    # (B, hidden)\n",
    "\n",
    "\n",
    "class CrossAttnWithHead(nn.Module):\n",
    "    \"\"\"Encoder + 小回归头（只用于训练 encoder）\"\"\"\n",
    "    def __init__(self, dim_a, dim_b,\n",
    "                 hidden_dim=256,\n",
    "                 num_heads=4,\n",
    "                 mlp_hidden=512,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttnEncoder(dim_a, dim_b, hidden_dim, num_heads, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, xa, xb):\n",
    "        fused = self.encoder(xa, xb)          # (B, hidden)\n",
    "        out   = self.head(fused).squeeze(-1)  # (B,)\n",
    "        return out, fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b43152-c670-4897-a528-33691638f3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bc1a94-5645-473c-966d-283641c6591a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attn train 样本数: 2581\n",
      "Cross-Attn val   样本数: 632\n",
      "X_text_ca_train 形状: (2581, 768)\n",
      "X_graph_ca_train形状: (2581, 256)\n",
      "X_text_ca_val   形状: (632, 768)\n",
      "X_graph_ca_val  形状: (632, 256)\n",
      "[Cross-Attn T+G] Epoch 001 | train MAE = 0.6559, val MAE = 0.5221\n",
      "[Cross-Attn T+G] Epoch 002 | train MAE = 0.5275, val MAE = 0.5327\n",
      "[Cross-Attn T+G] Epoch 003 | train MAE = 0.5052, val MAE = 0.5506\n",
      "[Cross-Attn T+G] Epoch 004 | train MAE = 0.4947, val MAE = 0.5321\n",
      "[Cross-Attn T+G] Epoch 005 | train MAE = 0.4934, val MAE = 0.5146\n",
      "[Cross-Attn T+G] Epoch 006 | train MAE = 0.4810, val MAE = 0.5168\n",
      "[Cross-Attn T+G] Epoch 007 | train MAE = 0.4804, val MAE = 0.5463\n",
      "[Cross-Attn T+G] Epoch 008 | train MAE = 0.4691, val MAE = 0.5247\n",
      "[Cross-Attn T+G] Epoch 009 | train MAE = 0.4659, val MAE = 0.5351\n",
      "[Cross-Attn T+G] Epoch 010 | train MAE = 0.4538, val MAE = 0.5423\n",
      "[Cross-Attn T+G] Epoch 011 | train MAE = 0.4576, val MAE = 0.5026\n",
      "[Cross-Attn T+G] Epoch 012 | train MAE = 0.4547, val MAE = 0.5320\n",
      "[Cross-Attn T+G] Epoch 013 | train MAE = 0.4392, val MAE = 0.5124\n",
      "[Cross-Attn T+G] Epoch 014 | train MAE = 0.4373, val MAE = 0.5249\n",
      "[Cross-Attn T+G] Epoch 015 | train MAE = 0.4365, val MAE = 0.5508\n",
      "[Cross-Attn T+G] Epoch 016 | train MAE = 0.4339, val MAE = 0.5131\n",
      "[Cross-Attn T+G] Epoch 017 | train MAE = 0.4369, val MAE = 0.5128\n",
      "[Cross-Attn T+G] Epoch 018 | train MAE = 0.4268, val MAE = 0.5460\n",
      "[Cross-Attn T+G] Epoch 019 | train MAE = 0.4223, val MAE = 0.5224\n",
      "[Cross-Attn T+G] Epoch 020 | train MAE = 0.4159, val MAE = 0.4991\n",
      "[Cross-Attn T+G] Epoch 021 | train MAE = 0.4200, val MAE = 0.5002\n",
      "[Cross-Attn T+G] Epoch 022 | train MAE = 0.4211, val MAE = 0.5188\n",
      "[Cross-Attn T+G] Epoch 023 | train MAE = 0.4172, val MAE = 0.5170\n",
      "[Cross-Attn T+G] Epoch 024 | train MAE = 0.4079, val MAE = 0.5103\n",
      "[Cross-Attn T+G] Epoch 025 | train MAE = 0.4144, val MAE = 0.5246\n",
      "[Cross-Attn T+G] Epoch 026 | train MAE = 0.4025, val MAE = 0.5124\n",
      "[Cross-Attn T+G] Epoch 027 | train MAE = 0.4037, val MAE = 0.5267\n",
      "[Cross-Attn T+G] Epoch 028 | train MAE = 0.3994, val MAE = 0.5284\n",
      "[Cross-Attn T+G] Epoch 029 | train MAE = 0.3973, val MAE = 0.5586\n",
      "[Cross-Attn T+G] Epoch 030 | train MAE = 0.4006, val MAE = 0.5310\n",
      "[Cross-Attn T+G] Early stopping at epoch 30, best_epoch = 20\n",
      "fused_all_TG 形状: (3213, 256)\n",
      "\n",
      "✅ Cross-Attn (Text+Graph) 训练完成，fused_all_TG 已保存。\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: Cross-Attn 按 SMILES 8:2 划分 train/val & 训练 =====\n",
    "\n",
    "# 1) 按 SMILES 分组，8:2 给 Cross-Attn 用（注意：这是 Cross-Attn 自己的一套划分）\n",
    "gss_ca = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=2025)\n",
    "N = len(y_all)\n",
    "idx_all = np.arange(N)\n",
    "\n",
    "ca_train_idx, ca_val_idx = next(\n",
    "    gss_ca.split(np.zeros(N), y_all, groups_all)\n",
    ")\n",
    "ca_train_idx = np.array(ca_train_idx, dtype=np.int64)\n",
    "ca_val_idx   = np.array(ca_val_idx, dtype=np.int64)\n",
    "\n",
    "print(\"Cross-Attn train 样本数:\", len(ca_train_idx))\n",
    "print(\"Cross-Attn val   样本数:\", len(ca_val_idx))\n",
    "\n",
    "np.save(OUT_TG / \"ca_train_idx_T_G.npy\", ca_train_idx)\n",
    "np.save(OUT_TG / \"ca_val_idx_T_G.npy\",   ca_val_idx)\n",
    "\n",
    "# 2) 在 Cross-Attn 的 train80% 上拟合 scaler（Text / Graph）\n",
    "scaler_text_ca  = StandardScaler().fit(X_text_raw[ca_train_idx])\n",
    "scaler_graph_ca = StandardScaler().fit(X_graph_raw[ca_train_idx])\n",
    "\n",
    "X_text_ca_all_std  = scaler_text_ca.transform(X_text_raw)\n",
    "X_graph_ca_all_std = scaler_graph_ca.transform(X_graph_raw)\n",
    "\n",
    "X_text_ca_train = X_text_ca_all_std[ca_train_idx]\n",
    "X_graph_ca_train= X_graph_ca_all_std[ca_train_idx]\n",
    "y_ca_train      = y_all[ca_train_idx]\n",
    "\n",
    "X_text_ca_val   = X_text_ca_all_std[ca_val_idx]\n",
    "X_graph_ca_val  = X_graph_ca_all_std[ca_val_idx]\n",
    "y_ca_val        = y_all[ca_val_idx]\n",
    "\n",
    "print(\"X_text_ca_train 形状:\", X_text_ca_train.shape)\n",
    "print(\"X_graph_ca_train形状:\", X_graph_ca_train.shape)\n",
    "print(\"X_text_ca_val   形状:\", X_text_ca_val.shape)\n",
    "print(\"X_graph_ca_val  形状:\", X_graph_ca_val.shape)\n",
    "\n",
    "# 3) 构建 DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "ds_ca_tr  = PairDataset(X_text_ca_train, X_graph_ca_train, y_ca_train)\n",
    "ds_ca_val = PairDataset(X_text_ca_val,  X_graph_ca_val,  y_ca_val)\n",
    "\n",
    "dl_ca_tr  = DataLoader(ds_ca_tr,  batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "dl_ca_val = DataLoader(ds_ca_val, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# 4) 定义 Cross-Attn 模型\n",
    "dim_a = X_text_ca_train.shape[1]\n",
    "dim_b = X_graph_ca_train.shape[1]\n",
    "\n",
    "model_ca = CrossAttnWithHead(\n",
    "    dim_a=dim_a,\n",
    "    dim_b=dim_b,\n",
    "    hidden_dim=256,\n",
    "    num_heads=4,\n",
    "    mlp_hidden=512,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_ca.parameters(),\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "max_epochs = 80\n",
    "patience   = 10\n",
    "best_val_mae = float(\"inf\")\n",
    "best_state_dict = None\n",
    "best_epoch = -1\n",
    "epochs_no_improve = 0\n",
    "\n",
    "history_ca = {\"train_mae\": [], \"val_mae\": []}\n",
    "\n",
    "# 5) 训练（用 val 做 early stopping）\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model_ca.train()\n",
    "    train_abs_err = []\n",
    "\n",
    "    for X1_b, X2_b, y_b in dl_ca_tr:\n",
    "        X1_b = X1_b.to(device)\n",
    "        X2_b = X2_b.to(device)\n",
    "        y_b  = y_b.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, fused = model_ca(X1_b, X2_b)\n",
    "        loss = loss_fn(y_hat, y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_ca.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_abs_err.append(torch.abs(y_hat.detach() - y_b).cpu().numpy())\n",
    "\n",
    "    train_mae = float(np.mean(np.concatenate(train_abs_err)))\n",
    "    history_ca[\"train_mae\"].append(train_mae)\n",
    "\n",
    "    # 验证\n",
    "    model_ca.eval()\n",
    "    val_abs_err = []\n",
    "    with torch.no_grad():\n",
    "        for X1_b, X2_b, y_b in dl_ca_val:\n",
    "            X1_b = X1_b.to(device)\n",
    "            X2_b = X2_b.to(device)\n",
    "            y_b  = y_b.to(device)\n",
    "\n",
    "            y_hat, fused = model_ca(X1_b, X2_b)\n",
    "            val_abs_err.append(torch.abs(y_hat - y_b).cpu().numpy())\n",
    "\n",
    "    val_mae = float(np.mean(np.concatenate(val_abs_err)))\n",
    "    history_ca[\"val_mae\"].append(val_mae)\n",
    "\n",
    "    print(f\"[Cross-Attn T+G] Epoch {epoch:03d} | train MAE = {train_mae:.4f}, val MAE = {val_mae:.4f}\")\n",
    "\n",
    "    if val_mae < best_val_mae - 1e-4:\n",
    "        best_val_mae = val_mae\n",
    "        best_state_dict = {k: v.cpu().clone() for k, v in model_ca.state_dict().items()}\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"[Cross-Attn T+G] Early stopping at epoch {epoch}, best_epoch = {best_epoch}\")\n",
    "            break\n",
    "\n",
    "# 6) 加载最佳权重，抽出 encoder，对“所有样本”生成 fused_all_TG\n",
    "if best_state_dict is not None:\n",
    "    model_ca.load_state_dict(best_state_dict)\n",
    "model_ca.to(device)\n",
    "model_ca.eval()\n",
    "\n",
    "encoder_tg = model_ca.encoder  # 只取 encoder\n",
    "encoder_tg.eval().to(device)\n",
    "\n",
    "ds_all_ca = PairDataset(X_text_ca_all_std, X_graph_ca_all_std, y_all)\n",
    "dl_all_ca = DataLoader(ds_all_ca, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "fused_all_list = []\n",
    "with torch.no_grad():\n",
    "    for X1_b, X2_b, y_b in dl_all_ca:\n",
    "        X1_b = X1_b.to(device)\n",
    "        X2_b = X2_b.to(device)\n",
    "        fused = encoder_tg(X1_b, X2_b)  # (B, hidden_dim)\n",
    "        fused_all_list.append(fused.cpu().numpy())\n",
    "\n",
    "fused_all_TG = np.concatenate(fused_all_list, axis=0)  # (N, hidden_dim)\n",
    "print(\"fused_all_TG 形状:\", fused_all_TG.shape)\n",
    "\n",
    "# 保存 Cross-Attn 模型 & fused_all\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": best_state_dict,\n",
    "        \"config\": {\n",
    "            \"dim_a\": dim_a,\n",
    "            \"dim_b\": dim_b,\n",
    "            \"hidden_dim\": 256,\n",
    "            \"num_heads\": 4,\n",
    "            \"mlp_hidden\": 512,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    MODELS_CA / \"crossattn_T_G_best.pt\"\n",
    ")\n",
    "\n",
    "np.save(OUT_TG / \"fused_all_T_G.npy\", fused_all_TG)\n",
    "np.save(OUT_TG / \"row_id_all_T_G.npy\", rowid_all)\n",
    "np.save(OUT_TG / \"y_all_T_G.npy\",      y_all)\n",
    "np.save(OUT_TG / \"groups_all_T_G.npy\", groups_all)\n",
    "\n",
    "with open(OUT_TG / \"crossattn_T_G_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_val_mae\": float(best_val_mae),\n",
    "            \"history\": history_ca,\n",
    "            \"n_all\": int(len(y_all)),\n",
    "            \"n_train\": int(len(ca_train_idx)),\n",
    "            \"n_val\": int(len(ca_val_idx)),\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "        default=np_encoder,\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Cross-Attn (Text+Graph) 训练完成，fused_all_TG 已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06679b7c-5256-4cc6-bc87-c7591027561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c820f02-3315-4346-a25f-519ad09f2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train 样本数: 2553\n",
      "RF test  样本数: 660\n",
      "dur_all_std_rf 形状: (3213, 1)\n",
      "X_all_RF 形状: (3213, 260)\n",
      "X_train_RF 形状: (2553, 260)\n",
      "X_test_RF  形状: (660, 260)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: RF 端 8:2 划分（按 SMILES 分组，但独立于 Cross-Attn） =====\n",
    "\n",
    "# 1) 先在 RF 端自己的 80% 上拟合 Duration 的 scaler\n",
    "gss_rf = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=2027)  # 不同 random_state\n",
    "N = len(y_all)\n",
    "rf_train_idx, rf_test_idx = next(\n",
    "    gss_rf.split(np.zeros(N), y_all, groups_all)\n",
    ")\n",
    "rf_train_idx = np.array(rf_train_idx, dtype=np.int64)\n",
    "rf_test_idx  = np.array(rf_test_idx, dtype=np.int64)\n",
    "\n",
    "print(\"RF train 样本数:\", len(rf_train_idx))\n",
    "print(\"RF test  样本数:\", len(rf_test_idx))\n",
    "\n",
    "np.save(OUT_TG / \"rf_train_idx_T_G.npy\", rf_train_idx)\n",
    "np.save(OUT_TG / \"rf_test_idx_T_G.npy\",  rf_test_idx)\n",
    "\n",
    "# RF 的数值特征：Duration 标准化（只在 RF 的 train80% 上 fit）\n",
    "scaler_dur_rf = StandardScaler().fit(dur_raw[rf_train_idx])\n",
    "dur_all_std_rf = scaler_dur_rf.transform(dur_raw)\n",
    "\n",
    "print(\"dur_all_std_rf 形状:\", dur_all_std_rf.shape)\n",
    "\n",
    "# 2) 构造 RF 用的最终特征： [ fused_all_TG , dur_all_std_rf , cat_all ]\n",
    "X_all_RF = np.concatenate(\n",
    "    [fused_all_TG, dur_all_std_rf, cat_all],\n",
    "    axis=1\n",
    ")\n",
    "print(\"X_all_RF 形状:\", X_all_RF.shape)\n",
    "\n",
    "X_train_RF = X_all_RF[rf_train_idx]\n",
    "y_train_RF = y_all[rf_train_idx]\n",
    "groups_train_RF = groups_all[rf_train_idx]\n",
    "\n",
    "X_test_RF  = X_all_RF[rf_test_idx]\n",
    "y_test_RF  = y_all[rf_test_idx]\n",
    "\n",
    "print(\"X_train_RF 形状:\", X_train_RF.shape)\n",
    "print(\"X_test_RF  形状:\", X_test_RF.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0a40d-e237-4227-b128-a7c4e2829d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5053c75e-4abb-4676-b97f-4ccf119ea193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始在 RF 80% train 上做十折 GroupKFold 随机超参搜索（T+G fused + meta）...\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "\n",
      "[T+G+meta→RF] 最优超参（基于 RF 80% train 十折CV）：\n",
      "{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.3, 'max_depth': 30}\n",
      "最优 CV 分数 (neg MAE): -0.40945305363846163\n",
      "\n",
      "===== [T+G fused + duration/meta → RF] RF train80% 指标 =====\n",
      "MAE: 0.2066\n",
      "RMSE: 0.3128\n",
      "R2: 0.9294\n",
      "Pearson_r: 0.9659\n",
      "\n",
      "===== [T+G fused + duration/meta → RF] RF test20% 指标 =====\n",
      "MAE: 0.3938\n",
      "RMSE: 0.5549\n",
      "R2: 0.7497\n",
      "Pearson_r: 0.8671\n",
      "\n",
      "✅ RF 训练 & 评估完成，模型与指标已保存。\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8: RF 十折 CV + RandomizedSearchCV 在 RF 80% train 上进行 =====\n",
    "\n",
    "# 1) RF 超参空间\n",
    "param_distributions_rf = {\n",
    "    \"n_estimators\":      [200, 300, 500, 800, 1000],\n",
    "    \"max_depth\":         [None, 10, 20, 30, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\":  [1, 2, 4],\n",
    "    \"max_features\":      [\"sqrt\", \"log2\", 0.3, 0.5, 0.8],\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    random_state=GLOBAL_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "cv_inner = GroupKFold(n_splits=10)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    random_state=GLOBAL_SEED,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"开始在 RF 80% train 上做十折 GroupKFold 随机超参搜索（T+G fused + meta）...\")\n",
    "rf_search.fit(X_train_RF, y_train_RF, groups=groups_train_RF)\n",
    "\n",
    "best_params_rf = rf_search.best_params_\n",
    "best_score_rf  = rf_search.best_score_\n",
    "\n",
    "print(\"\\n[T+G+meta→RF] 最优超参（基于 RF 80% train 十折CV）：\")\n",
    "print(best_params_rf)\n",
    "print(\"最优 CV 分数 (neg MAE):\", best_score_rf)\n",
    "\n",
    "# 2) 用最优超参在 RF train80% 上重训 RF\n",
    "rf_final = RandomForestRegressor(\n",
    "    **best_params_rf,\n",
    "    random_state=GLOBAL_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_final.fit(X_train_RF, y_train_RF)\n",
    "\n",
    "# 3) 评估：train80% & test20%\n",
    "y_train_pred_RF = rf_final.predict(X_train_RF)\n",
    "y_test_pred_RF  = rf_final.predict(X_test_RF)\n",
    "\n",
    "metrics_train_RF = compute_regression_metrics(y_train_RF, y_train_pred_RF)\n",
    "metrics_test_RF  = compute_regression_metrics(y_test_RF,  y_test_pred_RF)\n",
    "\n",
    "print(\"\\n===== [T+G fused + duration/meta → RF] RF train80% 指标 =====\")\n",
    "for k, v in metrics_train_RF.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== [T+G fused + duration/meta → RF] RF test20% 指标 =====\")\n",
    "for k, v in metrics_test_RF.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# 4) 保存 RF 模型 & 结果\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": rf_final,\n",
    "        \"scaler_text_ca\": scaler_text_ca,\n",
    "        \"scaler_graph_ca\": scaler_graph_ca,\n",
    "        \"encoder_state_dict\": best_state_dict,\n",
    "        \"scaler_dur_rf\": scaler_dur_rf,\n",
    "        \"cat_feature_names\": cat_feature_names,\n",
    "        \"config\": {\n",
    "            \"hidden_dim\": 256,\n",
    "            \"GLOBAL_SEED\": int(GLOBAL_SEED),\n",
    "            \"param_distributions_rf\": param_distributions_rf,\n",
    "        },\n",
    "    },\n",
    "    MODELS_RF / \"rf_T_G_meta_from_CA.joblib\"\n",
    ")\n",
    "\n",
    "np.save(OUT_TG / \"X_train_RF_T_G.npy\", X_train_RF)\n",
    "np.save(OUT_TG / \"X_test_RF_T_G.npy\",  X_test_RF)\n",
    "np.save(OUT_TG / \"y_train_RF_T_G.npy\", y_train_RF)\n",
    "np.save(OUT_TG / \"y_test_RF_T_G.npy\",  y_test_RF)\n",
    "np.save(OUT_TG / \"y_train_pred_RF_T_G.npy\", y_train_pred_RF)\n",
    "np.save(OUT_TG / \"y_test_pred_RF_T_G.npy\",  y_test_pred_RF)\n",
    "\n",
    "with open(OUT_TG / \"metrics_RF_T_G_meta_from_CA.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_params_rf\": best_params_rf,\n",
    "            \"best_score_cv_neg_mae\": float(best_score_rf),\n",
    "            \"train80_metrics\": metrics_train_RF,\n",
    "            \"test20_metrics\": metrics_test_RF,\n",
    "            \"n_all\": int(len(y_all)),\n",
    "            \"n_train80_rf\": int(len(y_train_RF)),\n",
    "            \"n_test20_rf\": int(len(y_test_RF)),\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "        default=np_encoder,\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ RF 训练 & 评估完成，模型与指标已保存。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
