{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3505dc50-5e6a-4872-8d11-76dec453f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: 导入 & 工具函数 =====\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr, randint\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return {\n",
    "        \"r2\":   float(r2_score(y_true, y_pred)),\n",
    "        \"mae\":  float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"r\":    float(pearsonr(y_true, y_pred)[0]),\n",
    "    }\n",
    "\n",
    "def np_encoder(o):\n",
    "    if isinstance(o, (np.integer,)):  return int(o)\n",
    "    if isinstance(o, (np.floating,)): return float(o)\n",
    "    if isinstance(o, np.ndarray):     return o.tolist()\n",
    "    raise TypeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234867dc-9d22-4deb-a702-ee681956cf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a226e5d7-e193-4987-98a2-8b9600670c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mgperL  mgperL_log\n",
      "0     1.3    0.113943\n",
      "1     2.5    0.397940\n",
      "2    40.8    1.610660\n",
      "3     1.9    0.278754\n",
      "4     0.6   -0.221849\n",
      "总样本数: 3620\n",
      "有效标签数(非 NaN): 3620\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: 读原始数据 + 构建 log10(mg/L) 标签 =====\n",
    "\n",
    "ROOT_MULTI = Path(\"/root/Invertebrates_EC50_multi_fusion\")\n",
    "\n",
    "DATA_PATH  = Path(\"/root/fusion_dataset/with_physchem_excels/Invertebrates_EC50_unique_physchem.xlsx\")\n",
    "SMILES_COL = \"SMILES_Canonical_RDKit\"\n",
    "\n",
    "LABEL_RAW  = \"mgperL\"       # 原始浓度列\n",
    "LABEL_LOG  = \"mgperL_log\"   # 我们现在自己构建\n",
    "\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 原始浓度转成数值\n",
    "df[LABEL_RAW] = pd.to_numeric(df[LABEL_RAW], errors=\"coerce\")\n",
    "\n",
    "# 只对 >0 的样本做 log10，其他设为 NaN\n",
    "mask_pos = df[LABEL_RAW] > 0\n",
    "df[LABEL_LOG] = np.where(mask_pos, np.log10(df[LABEL_RAW]), np.nan)\n",
    "\n",
    "print(df[[LABEL_RAW, LABEL_LOG]].head())\n",
    "\n",
    "y_all_full      = df[LABEL_LOG].values          # log10(mg/L)\n",
    "groups_all_full = df[SMILES_COL].astype(str).values\n",
    "\n",
    "print(\"总样本数:\", len(df))\n",
    "print(\"有效标签数(非 NaN):\", np.isfinite(y_all_full).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d8e64-4ce3-40c6-a61e-c5aaa40b8436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cf9777-2112-4ad3-991f-7bcb25ba0d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_text_all 形状: (3620, 768)\n",
      "X_graph_raw 形状: (3213, 256)\n",
      "rowid_graph 范围: 1 → 3619\n",
      "X_phys_raw 形状: (3406, 64)\n",
      "rowid_phys 范围: 0 → 3619\n",
      "三模态交集样本数: 3103\n",
      "对齐后 X_text: (3103, 768)\n",
      "对齐后 X_graph: (3103, 256)\n",
      "对齐后 X_phys: (3103, 64)\n",
      "y_all 形状: (3103,)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: 加载 Text / Graph / PhysChem 嵌入，并按 df 行号对齐 =====\n",
    "\n",
    "# ---- Text 端 CLS embedding ----\n",
    "SMILES_OUT_DIR = ROOT_MULTI / \"SMILES\" / \"smiles_outputs\"\n",
    "TEXT_EMB_PATH  = SMILES_OUT_DIR / \"reg_smiles_cls_embeddings_all.npy\"  # 按你实际文件名改\n",
    "\n",
    "X_text_all = np.load(TEXT_EMB_PATH)   # (N_df, d_text=768)\n",
    "rowid_text = np.arange(len(df), dtype=int)\n",
    "\n",
    "print(\"X_text_all 形状:\", X_text_all.shape)\n",
    "\n",
    "# ---- Graph 端 GNN embedding ----\n",
    "GRAPH_OUT_DIR    = ROOT_MULTI / \"graph\" / \"graph_outputs\"\n",
    "GRAPH_EMB_PATH   = GRAPH_OUT_DIR / \"reg_graph_embeddings.npy\"          # (N_graph, d_graph)\n",
    "GRAPH_ROWID_PATH = GRAPH_OUT_DIR / \"row_id_graph_for_emb.npy\"          # df 行号\n",
    "\n",
    "X_graph_raw = np.load(GRAPH_EMB_PATH)\n",
    "rowid_graph = np.load(GRAPH_ROWID_PATH).astype(int)\n",
    "\n",
    "print(\"X_graph_raw 形状:\", X_graph_raw.shape)\n",
    "print(\"rowid_graph 范围:\", rowid_graph.min(), \"→\", rowid_graph.max())\n",
    "\n",
    "# ---- PhysChem 端 MLP embedding ----\n",
    "PHY_OUT_DIR    = ROOT_MULTI / \"phychem\" / \"physchem_mlp_rf_v2\"\n",
    "PHY_EMB_PATH   = PHY_OUT_DIR / \"emb_physchem_mlp_all.npy\"  # descMLP 保存的 embedding\n",
    "PHY_ROWID_PATH = PHY_OUT_DIR / \"row_id_clean.npy\"          # 对应 df 行号\n",
    "\n",
    "X_phys_raw = np.load(PHY_EMB_PATH)\n",
    "rowid_phys = np.load(PHY_ROWID_PATH).astype(int)\n",
    "\n",
    "print(\"X_phys_raw 形状:\", X_phys_raw.shape)\n",
    "print(\"rowid_phys 范围:\", rowid_phys.min(), \"→\", rowid_phys.max())\n",
    "\n",
    "# ---- 三模态 row_id 交集 ----\n",
    "ids_text  = set(rowid_text.tolist())\n",
    "ids_graph = set(rowid_graph.tolist())\n",
    "ids_phys  = set(rowid_phys.tolist())\n",
    "\n",
    "ids_inter = sorted(list(ids_text & ids_graph & ids_phys))\n",
    "print(\"三模态交集样本数:\", len(ids_inter))\n",
    "\n",
    "idx_map_graph = {rid: i for i, rid in enumerate(rowid_graph)}\n",
    "idx_map_phys  = {rid: i for i, rid in enumerate(rowid_phys)}\n",
    "\n",
    "X_text_list, X_graph_list, X_phys_list = [], [], []\n",
    "y_list, groups_list, rid_list = [], [], []\n",
    "\n",
    "for rid in ids_inter:\n",
    "    y_val = y_all_full[rid]\n",
    "    if not np.isfinite(y_val):\n",
    "        continue  # 把 mgperL<=0 或缺失的样本丢掉\n",
    "\n",
    "    X_text_list.append(X_text_all[rid])\n",
    "    X_graph_list.append(X_graph_raw[idx_map_graph[rid]])\n",
    "    X_phys_list.append(X_phys_raw[idx_map_phys[rid]])\n",
    "\n",
    "    y_list.append(y_all_full[rid])\n",
    "    groups_list.append(groups_all_full[rid])\n",
    "    rid_list.append(rid)\n",
    "\n",
    "X_text = np.stack(X_text_list, axis=0)   # (N, d_text)\n",
    "X_graph= np.stack(X_graph_list, axis=0)  # (N, d_graph)\n",
    "X_phys = np.stack(X_phys_list, axis=0)   # (N, d_phys)\n",
    "y_all  = np.array(y_list, dtype=float)   # (N,)\n",
    "groups = np.array(groups_list)           # (N,)\n",
    "rowid  = np.array(rid_list, dtype=int)   # (N,)\n",
    "\n",
    "print(\"对齐后 X_text:\", X_text.shape)\n",
    "print(\"对齐后 X_graph:\", X_graph.shape)\n",
    "print(\"对齐后 X_phys:\", X_phys.shape)\n",
    "print(\"y_all 形状:\", y_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39d629-be0a-4d9c-8a60-ffd2345b1be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d42e5f6-03da-4298-8275-e6051892c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: 定义三模态 Cross-Attn 模型 =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用设备:\", device)\n",
    "\n",
    "class TriModalDataset(Dataset):\n",
    "    def __init__(self, X_text, X_graph, X_phys, y):\n",
    "        self.X_text  = torch.from_numpy(X_text).float()\n",
    "        self.X_graph = torch.from_numpy(X_graph).float()\n",
    "        self.X_phys  = torch.from_numpy(X_phys).float()\n",
    "        self.y       = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X_text[idx],\n",
    "            self.X_graph[idx],\n",
    "            self.X_phys[idx],\n",
    "            self.y[idx],\n",
    "        )\n",
    "\n",
    "class TriCrossAttnRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    三模态 Cross-Attn：\n",
    "    - 输入: text_emb (B, d_t), graph_emb (B, d_g), phys_emb (B, d_p)\n",
    "    - 先线性投影到同一维度 hidden_dim\n",
    "    - 堆成 3 个 token 做 multi-head self-attention\n",
    "    - 对 3 个 token 做平均池化 → fused_emb\n",
    "    - fused_emb → 小 MLP 输出 y\n",
    "    - 可返回 fused_emb 用作中期融合 embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_text, dim_graph, dim_phys,\n",
    "                 hidden_dim=256, num_heads=4, mlp_hidden=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.text_proj  = nn.Linear(dim_text,  hidden_dim)\n",
    "        self.graph_proj = nn.Linear(dim_graph, hidden_dim)\n",
    "        self.phys_proj  = nn.Linear(dim_phys,  hidden_dim)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,  # (S, B, E)\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_text, x_graph, x_phys, return_emb=False, return_attn=False):\n",
    "        \"\"\"\n",
    "        x_text : (B, d_t)\n",
    "        x_graph: (B, d_g)\n",
    "        x_phys : (B, d_p)\n",
    "        \"\"\"\n",
    "\n",
    "        t = self.text_proj(x_text)   # (B, H)\n",
    "        g = self.graph_proj(x_graph) # (B, H)\n",
    "        p = self.phys_proj(x_phys)   # (B, H)\n",
    "\n",
    "        # tokens: (S=3, B, H)\n",
    "        tokens = torch.stack([t, g, p], dim=0)\n",
    "\n",
    "        # attn_w: (B, 3, 3)  (batch, tgt_len, src_len)\n",
    "        attn_out, attn_w = self.attn(\n",
    "            tokens, tokens, tokens,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=True\n",
    "        )\n",
    "\n",
    "        fused = attn_out.mean(dim=0)           # (B, H)\n",
    "        y_pred = self.mlp(fused).squeeze(-1)   # (B,)\n",
    "\n",
    "        if return_attn:\n",
    "            # 输入 token（=模态）的总贡献：对 query(tgt) 维取平均 -> (B, 3)，且每行和=1\n",
    "            contrib = attn_w.mean(dim=1)  # (B,3): [text, graph, phys]\n",
    "            if return_emb:\n",
    "                return y_pred, fused, contrib, attn_w\n",
    "            return y_pred, contrib, attn_w\n",
    "\n",
    "        if return_emb:\n",
    "            return y_pred, fused\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# 设定 Cross-Attn 超参（可以以后再调）\n",
    "CA_HIDDEN_DIM   = 256\n",
    "CA_NUM_HEADS    = 4\n",
    "CA_MLP_HIDDEN   = 512\n",
    "CA_DROPOUT      = 0.1\n",
    "CA_LR           = 5e-4\n",
    "CA_WEIGHT_DECAY = 1e-4\n",
    "CA_BATCH_SIZE   = 64\n",
    "CA_EPOCHS       = 50\n",
    "\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ffcfe-4c19-4dd3-a63a-e2b257e47cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2664e9d1-677a-41b5-a4aa-37de22ade6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attn train 样本数: 2487\n",
      "Cross-Attn val   样本数: 616\n",
      "[Epoch 001] train_L1 = 0.6766, val_L1 = 0.5733\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.5733\n",
      "[Epoch 002] train_L1 = 0.5173, val_L1 = 0.5107\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.5107\n",
      "[Epoch 003] train_L1 = 0.5040, val_L1 = 0.5394\n",
      "[Epoch 004] train_L1 = 0.5040, val_L1 = 0.4954\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4954\n",
      "[Epoch 005] train_L1 = 0.4868, val_L1 = 0.5026\n",
      "[Epoch 006] train_L1 = 0.4865, val_L1 = 0.4785\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4785\n",
      "[Epoch 007] train_L1 = 0.4651, val_L1 = 0.4930\n",
      "[Epoch 008] train_L1 = 0.4750, val_L1 = 0.4938\n",
      "[Epoch 009] train_L1 = 0.4698, val_L1 = 0.4990\n",
      "[Epoch 010] train_L1 = 0.4636, val_L1 = 0.4853\n",
      "[Epoch 011] train_L1 = 0.4552, val_L1 = 0.5110\n",
      "[Epoch 012] train_L1 = 0.4692, val_L1 = 0.5025\n",
      "[Epoch 013] train_L1 = 0.4649, val_L1 = 0.4858\n",
      "[Epoch 014] train_L1 = 0.4448, val_L1 = 0.4929\n",
      "[Epoch 015] train_L1 = 0.4364, val_L1 = 0.4760\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4760\n",
      "[Epoch 016] train_L1 = 0.4385, val_L1 = 0.4656\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4656\n",
      "[Epoch 017] train_L1 = 0.4354, val_L1 = 0.4684\n",
      "[Epoch 018] train_L1 = 0.4422, val_L1 = 0.4683\n",
      "[Epoch 019] train_L1 = 0.4260, val_L1 = 0.4755\n",
      "[Epoch 020] train_L1 = 0.4328, val_L1 = 0.4650\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4650\n",
      "[Epoch 021] train_L1 = 0.4263, val_L1 = 0.4917\n",
      "[Epoch 022] train_L1 = 0.4274, val_L1 = 0.4710\n",
      "[Epoch 023] train_L1 = 0.4244, val_L1 = 0.4752\n",
      "[Epoch 024] train_L1 = 0.4325, val_L1 = 0.4866\n",
      "[Epoch 025] train_L1 = 0.4250, val_L1 = 0.5032\n",
      "[Epoch 026] train_L1 = 0.4250, val_L1 = 0.4900\n",
      "[Epoch 027] train_L1 = 0.4092, val_L1 = 0.4674\n",
      "[Epoch 028] train_L1 = 0.4093, val_L1 = 0.4863\n",
      "[Epoch 029] train_L1 = 0.4154, val_L1 = 0.4693\n",
      "[Epoch 030] train_L1 = 0.4035, val_L1 = 0.4741\n",
      "[Epoch 031] train_L1 = 0.4205, val_L1 = 0.4958\n",
      "[Epoch 032] train_L1 = 0.4069, val_L1 = 0.4643\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4643\n",
      "[Epoch 033] train_L1 = 0.4018, val_L1 = 0.4694\n",
      "[Epoch 034] train_L1 = 0.4219, val_L1 = 0.5134\n",
      "[Epoch 035] train_L1 = 0.4238, val_L1 = 0.4995\n",
      "[Epoch 036] train_L1 = 0.4050, val_L1 = 0.4868\n",
      "[Epoch 037] train_L1 = 0.3963, val_L1 = 0.5133\n",
      "[Epoch 038] train_L1 = 0.4003, val_L1 = 0.4824\n",
      "[Epoch 039] train_L1 = 0.4006, val_L1 = 0.4780\n",
      "[Epoch 040] train_L1 = 0.4016, val_L1 = 0.4768\n",
      "[Epoch 041] train_L1 = 0.3954, val_L1 = 0.4824\n",
      "[Epoch 042] train_L1 = 0.3910, val_L1 = 0.4724\n",
      "[Epoch 043] train_L1 = 0.3862, val_L1 = 0.4639\n",
      "  -> 更新 best_state, 当前 best_val_L1 = 0.4639\n",
      "[Epoch 044] train_L1 = 0.3957, val_L1 = 0.4856\n",
      "[Epoch 045] train_L1 = 0.3933, val_L1 = 0.4688\n",
      "[Epoch 046] train_L1 = 0.3845, val_L1 = 0.4704\n",
      "[Epoch 047] train_L1 = 0.3956, val_L1 = 0.4986\n",
      "[Epoch 048] train_L1 = 0.3883, val_L1 = 0.4775\n",
      "[Epoch 049] train_L1 = 0.3909, val_L1 = 0.4877\n",
      "[Epoch 050] train_L1 = 0.3931, val_L1 = 0.4690\n",
      "\n",
      "✅ 加载最优验证损失对应的模型参数\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5: Cross-Attn 8:2 划分 + 训练 =====\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# 1) 8:2 按 SMILES 分组划分（用于训练神经网络）\n",
    "gss_ca = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=2026)\n",
    "idx_all = np.arange(len(y_all))\n",
    "train_idx_ca, val_idx_ca = next(gss_ca.split(idx_all, y_all, groups))\n",
    "\n",
    "train_idx_ca = train_idx_ca.astype(int)\n",
    "val_idx_ca   = val_idx_ca.astype(int)\n",
    "\n",
    "print(\"Cross-Attn train 样本数:\", len(train_idx_ca))\n",
    "print(\"Cross-Attn val   样本数:\", len(val_idx_ca))\n",
    "\n",
    "X_text_train_ca  = X_text[train_idx_ca]\n",
    "X_graph_train_ca = X_graph[train_idx_ca]\n",
    "X_phys_train_ca  = X_phys[train_idx_ca]\n",
    "y_train_ca       = y_all[train_idx_ca]\n",
    "\n",
    "X_text_val_ca  = X_text[val_idx_ca]\n",
    "X_graph_val_ca = X_graph[val_idx_ca]\n",
    "X_phys_val_ca  = X_phys[val_idx_ca]\n",
    "y_val_ca       = y_all[val_idx_ca]\n",
    "\n",
    "train_ds = TriModalDataset(X_text_train_ca, X_graph_train_ca, X_phys_train_ca, y_train_ca)\n",
    "val_ds   = TriModalDataset(X_text_val_ca,   X_graph_val_ca,   X_phys_val_ca,   y_val_ca)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CA_BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CA_BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "dim_text  = X_text.shape[1]\n",
    "dim_graph = X_graph.shape[1]\n",
    "dim_phys  = X_phys.shape[1]\n",
    "\n",
    "model = TriCrossAttnRegressor(\n",
    "    dim_text=dim_text,\n",
    "    dim_graph=dim_graph,\n",
    "    dim_phys=dim_phys,\n",
    "    hidden_dim=CA_HIDDEN_DIM,\n",
    "    num_heads=CA_NUM_HEADS,\n",
    "    mlp_hidden=CA_MLP_HIDDEN,\n",
    "    dropout=CA_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CA_LR, weight_decay=CA_WEIGHT_DECAY)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, CA_EPOCHS + 1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    for x_t, x_g, x_p, y in train_loader:\n",
    "        x_t = x_t.to(device)\n",
    "        x_g = x_g.to(device)\n",
    "        x_p = x_p.to(device)\n",
    "        y   = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_t, x_g, x_p, return_emb=False)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / len(train_ds)\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_t, x_g, x_p, y in val_loader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_g = x_g.to(device)\n",
    "            x_p = x_p.to(device)\n",
    "            y   = y.to(device)\n",
    "\n",
    "            y_pred = model(x_t, x_g, x_p, return_emb=False)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_ds)\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}] train_L1 = {train_loss:.4f}, val_L1 = {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        print(f\"  -> 更新 best_state, 当前 best_val_L1 = {best_val_loss:.4f}\")\n",
    "\n",
    "# 加载最佳参数\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"\\n✅ 加载最优验证损失对应的模型参数\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4f2ed-11e0-4213-bb2c-137c928add15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c31b40-61f5-4bdd-beb5-a11192328f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_all 形状: (3103, 256)\n",
      "Cross-Attn 直接预测 y 形状: (3103,)\n",
      "\n",
      "✅ 三模态 Cross-Attn 融合 embedding 已保存到: /root/Invertebrates_EC50_multi_fusion/mid_fusion_TGP\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: 提取全体样本的三模态融合 embedding =====\n",
    "model.eval()\n",
    "\n",
    "fused_list = []\n",
    "y_pred_all_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    BATCH = 256\n",
    "    N_tot = len(y_all)\n",
    "    for start in range(0, N_tot, BATCH):\n",
    "        end = min(N_tot, start + BATCH)\n",
    "        x_t = torch.from_numpy(X_text[start:end]).float().to(device)\n",
    "        x_g = torch.from_numpy(X_graph[start:end]).float().to(device)\n",
    "        x_p = torch.from_numpy(X_phys[start:end]).float().to(device)\n",
    "\n",
    "        y_pred_batch, fused_batch = model(x_t, x_g, x_p, return_emb=True)\n",
    "        fused_list.append(fused_batch.cpu().numpy())\n",
    "        y_pred_all_list.append(y_pred_batch.cpu().numpy())\n",
    "\n",
    "fused_all   = np.concatenate(fused_list, axis=0)       # (N, hidden_dim)\n",
    "y_pred_all_ca = np.concatenate(y_pred_all_list, axis=0)  # 网络直接预测的 y (没用也可以保存一下)\n",
    "\n",
    "print(\"fused_all 形状:\", fused_all.shape)\n",
    "print(\"Cross-Attn 直接预测 y 形状:\", y_pred_all_ca.shape)\n",
    "\n",
    "MID_OUT_DIR = ROOT_MULTI / \"mid_fusion_TGP\"\n",
    "MID_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(MID_OUT_DIR / \"fused_emb_TGP_all.npy\", fused_all.astype(np.float32))\n",
    "np.save(MID_OUT_DIR / \"rowid_TGP.npy\", rowid.astype(np.int64))\n",
    "np.save(MID_OUT_DIR / \"y_all_TGP.npy\", y_all.astype(np.float32))\n",
    "np.save(MID_OUT_DIR / \"groups_TGP.npy\", groups.astype(\"U\"))\n",
    "\n",
    "# 保存 Cross-Attn 端预测（可选）\n",
    "np.save(MID_OUT_DIR / \"ca_y_pred_all_TGP.npy\", y_pred_all_ca.astype(np.float32))\n",
    "np.save(MID_OUT_DIR / \"train_idx_ca.npy\", train_idx_ca.astype(np.int64))\n",
    "np.save(MID_OUT_DIR / \"val_idx_ca.npy\",   val_idx_ca.astype(np.int64))\n",
    "\n",
    "# 保存模型参数\n",
    "torch.save(model.state_dict(), MID_OUT_DIR / \"tricrossattn_TGP_state_dict.pt\")\n",
    "\n",
    "print(\"\\n✅ 三模态 Cross-Attn 融合 embedding 已保存到:\", MID_OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b94c801-182c-4865-bc9b-24e0c6ce0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: /root/Invertebrates_EC50_multi_fusion/mid_fusion_TGP/mid_attn_contrib_TGP_all.npy\n",
      "✅ saved: /root/Invertebrates_EC50_multi_fusion/mid_fusion_TGP/mid_attn_contrib_TGP_all_report.json\n",
      "{'scope': 'ALL', 'mean': {'text': 0.18062035739421844, 'graph': 0.24020308256149292, 'phys': 0.5791768431663513}, 'std': {'text': 0.08111577481031418, 'graph': 0.10781783610582352, 'phys': 0.1305657923221588}}\n",
      "✅ saved: /root/Invertebrates_EC50_multi_fusion/mid_fusion_TGP/mid_attn_contrib_TGP_val.npy\n",
      "✅ saved: /root/Invertebrates_EC50_multi_fusion/mid_fusion_TGP/mid_attn_contrib_TGP_val_report.json\n",
      "{'scope': 'VAL', 'mean': {'text': 0.18288934230804443, 'graph': 0.24668678641319275, 'phys': 0.57042396068573}, 'std': {'text': 0.07997278869152069, 'graph': 0.10985127091407776, 'phys': 0.13516274094581604}}\n"
     ]
    }
   ],
   "source": [
    "def collect_attn_contrib(model, X_text, X_graph, X_phys, device, batch_size=256):\n",
    "    model.eval()\n",
    "    contrib_all = []\n",
    "\n",
    "    n = len(X_text)\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, n, batch_size):\n",
    "            e = min(n, s + batch_size)\n",
    "\n",
    "            xt = torch.from_numpy(X_text[s:e]).float().to(device)\n",
    "            xg = torch.from_numpy(X_graph[s:e]).float().to(device)\n",
    "            xp = torch.from_numpy(X_phys[s:e]).float().to(device)\n",
    "\n",
    "            # y_pred, fused, contrib, attn_w\n",
    "            _, _, contrib, _ = model(xt, xg, xp, return_emb=True, return_attn=True)\n",
    "            contrib_all.append(contrib.detach().cpu().numpy())\n",
    "\n",
    "    contrib_all = np.concatenate(contrib_all, axis=0)  # (N,3)\n",
    "    return contrib_all\n",
    "\n",
    "# ===== 1) 全体样本贡献 =====\n",
    "contrib_all = collect_attn_contrib(model, X_text, X_graph, X_phys, device, batch_size=256)\n",
    "\n",
    "mean_all = contrib_all.mean(axis=0)\n",
    "std_all  = contrib_all.std(axis=0)\n",
    "\n",
    "attn_report_all = {\n",
    "    \"scope\": \"ALL\",\n",
    "    \"mean\": {\"text\": float(mean_all[0]), \"graph\": float(mean_all[1]), \"phys\": float(mean_all[2])},\n",
    "    \"std\":  {\"text\": float(std_all[0]),  \"graph\": float(std_all[1]),  \"phys\": float(std_all[2])},\n",
    "}\n",
    "\n",
    "np.save(MID_OUT_DIR / \"mid_attn_contrib_TGP_all.npy\", contrib_all.astype(np.float32))\n",
    "with open(MID_OUT_DIR / \"mid_attn_contrib_TGP_all_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(attn_report_all, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ saved:\", MID_OUT_DIR / \"mid_attn_contrib_TGP_all.npy\")\n",
    "print(\"✅ saved:\", MID_OUT_DIR / \"mid_attn_contrib_TGP_all_report.json\")\n",
    "print(attn_report_all)\n",
    "\n",
    "# ===== 2) 验证集贡献（推荐看这个，更贴近泛化贡献）=====\n",
    "idx = val_idx_ca  # 你 notebook 里的 val index\n",
    "contrib_val = collect_attn_contrib(model, X_text[idx], X_graph[idx], X_phys[idx], device, batch_size=256)\n",
    "\n",
    "mean_val = contrib_val.mean(axis=0)\n",
    "std_val  = contrib_val.std(axis=0)\n",
    "\n",
    "attn_report_val = {\n",
    "    \"scope\": \"VAL\",\n",
    "    \"mean\": {\"text\": float(mean_val[0]), \"graph\": float(mean_val[1]), \"phys\": float(mean_val[2])},\n",
    "    \"std\":  {\"text\": float(std_val[0]),  \"graph\": float(std_val[1]),  \"phys\": float(std_val[2])},\n",
    "}\n",
    "\n",
    "np.save(MID_OUT_DIR / \"mid_attn_contrib_TGP_val.npy\", contrib_val.astype(np.float32))\n",
    "with open(MID_OUT_DIR / \"mid_attn_contrib_TGP_val_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(attn_report_val, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ saved:\", MID_OUT_DIR / \"mid_attn_contrib_TGP_val.npy\")\n",
    "print(\"✅ saved:\", MID_OUT_DIR / \"mid_attn_contrib_TGP_val_report.json\")\n",
    "print(attn_report_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e0d9d3-8b47-4f63-a04f-889ad44d1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数值型 meta 列: ['Duration_Value(hour)']\n",
      "类别型 meta 列: ['Effect', 'Endpoint']\n",
      "X_meta 形状: (3103, 4)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: 构建 meta 特征，用于 RF 中期融合 =====\n",
    "\n",
    "NUM_META_COLS_CANDIDATE = [\"Duration_Value(hour)\"]   # 数值型\n",
    "CAT_META_COLS_CANDIDATE = [\"Effect\", \"Endpoint\"]     # 类别型\n",
    "\n",
    "NUM_META_COLS = [c for c in NUM_META_COLS_CANDIDATE if c in df.columns]\n",
    "CAT_META_COLS = [c for c in CAT_META_COLS_CANDIDATE if c in df.columns]\n",
    "\n",
    "print(\"数值型 meta 列:\", NUM_META_COLS)\n",
    "print(\"类别型 meta 列:\", CAT_META_COLS)\n",
    "\n",
    "df_meta = df.loc[rowid].copy().reset_index(drop=True)\n",
    "\n",
    "# 数值型\n",
    "if NUM_META_COLS:\n",
    "    scaler_meta = StandardScaler()\n",
    "    X_num = scaler_meta.fit_transform(df_meta[NUM_META_COLS].values)\n",
    "else:\n",
    "    X_num = np.zeros((len(df_meta), 0), dtype=np.float32)\n",
    "\n",
    "# 类别型\n",
    "if CAT_META_COLS:\n",
    "    ohe_meta = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_cat = ohe_meta.fit_transform(df_meta[CAT_META_COLS].astype(str))\n",
    "else:\n",
    "    X_cat = np.zeros((len(df_meta), 0), dtype=np.float32)\n",
    "\n",
    "X_meta = np.concatenate([X_num, X_cat], axis=1)\n",
    "print(\"X_meta 形状:\", X_meta.shape)\n",
    "\n",
    "# 也把 meta 相关变换保存，方便后续复用\n",
    "with open(MID_OUT_DIR / \"meta_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_meta if NUM_META_COLS else None, f)\n",
    "\n",
    "with open(MID_OUT_DIR / \"meta_ohe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_meta if CAT_META_COLS else None, f)\n",
    "\n",
    "np.save(MID_OUT_DIR / \"X_meta_TGP_all.npy\", X_meta.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023688d-d373-46e8-8d44-7341f9fefa88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f9d647-3853-4c47-b633-feecd40fba54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 中期融合 train 样本数: 2455\n",
      "RF 中期融合 test  样本数: 648\n",
      "X_train_mid 形状: (2455, 260)\n",
      "X_test_mid  形状: (648, 260)\n",
      "\n",
      "==== [Mid T+G+P] RF 十折随机搜索 ====\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=13.8min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.6min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 7.1min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.9min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 1.9min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.9min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=19.0min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 8.5min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=22.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.5min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  30.0s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 7.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  59.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  56.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.8min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.7min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=16.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.7min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=18.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 5.5min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=22.2min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.7min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  29.5s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 6.3min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=18.5min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.6min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 7.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.8min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.1min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.1min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.5min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=24.9min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=20.2min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=10.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.0min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=14.1min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 6.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.7min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=19.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.1min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 4.9min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=22.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.8min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=15.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.7min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.5min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time= 9.7min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=15.6min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=19.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 9.6min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=23.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=13.8min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.2min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 7.7min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 3.6min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.6min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=17.7min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 8.7min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 5.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=22.8min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.3min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  26.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  49.3s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 8.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.5min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.6min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 6.9min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 6.5min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 1.9min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.1min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=24.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 9.0min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 4.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=10.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.2min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.8min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  29.7s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 7.4min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  56.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.1min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.2min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.5min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.2min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.0min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 8.9min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 4.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=10.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.1min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time=10.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=14.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.4min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 7.5min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 4.4min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 2.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=18.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.1min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 9.4min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 4.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 6.1min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.4min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=14.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.9min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=16.6min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.1min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=15.6min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=19.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 8.7min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=21.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.8min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=933; total time= 3.1min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  29.1s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 7.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  49.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.2min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.7min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.2min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=16.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 2.1min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.4min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=24.9min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=19.1min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.0min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  14.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  38.5s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 7.6min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  59.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.5min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.8min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.9min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=26.2min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.1min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 5.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.8min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=14.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.1min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 6.8min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 6.7min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=3, n_estimators=587; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.5min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.2min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.4min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 6.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 9.0min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=21.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 8.8min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  27.2s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 6.7min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=18.6min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.7min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=924; total time=16.6min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.4min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=356; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=720; total time=17.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.4min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 6.8min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 4.5min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=21.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.6min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=2, min_samples_split=9, n_estimators=691; total time=14.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 4.0min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.3min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.5min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=291; total time= 7.4min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 4.0min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=3, min_samples_split=9, n_estimators=234; total time= 4.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.7min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.2min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.7min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.4min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 5.3min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=21.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 8.9min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=3, min_samples_split=8, n_estimators=530; total time=  31.6s\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time= 7.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time=  50.7s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=360; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=760; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=4, n_estimators=766; total time= 9.4min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.5min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=16.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=7, n_estimators=676; total time= 1.9min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.7min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.1min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 8.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time=10.5min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.0min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 8.9min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time=16.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 3.4min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=3, min_samples_split=3, n_estimators=587; total time= 2.2min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.3min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.2min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.8min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=3, min_samples_split=2, n_estimators=591; total time= 7.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=8, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 4.8min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=3, min_samples_split=9, n_estimators=925; total time=21.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.2min\n",
      "[CV] END max_depth=20, max_features=0.5, min_samples_leaf=1, min_samples_split=9, n_estimators=900; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=4, min_samples_split=9, n_estimators=863; total time=17.8min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, n_estimators=846; total time= 4.0min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=976; total time=17.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=3, n_estimators=401; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=9, n_estimators=661; total time=10.6min\n",
      "[CV] END max_depth=30, max_features=0.8, min_samples_leaf=2, min_samples_split=6, n_estimators=991; total time=25.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=9, n_estimators=671; total time= 2.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=689; total time= 9.5min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=247; total time= 5.0min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=812; total time= 9.9min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 4.6min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=6, n_estimators=417; total time= 5.7min\n",
      "[CV] END max_depth=20, max_features=0.3, min_samples_leaf=1, min_samples_split=4, n_estimators=774; total time= 9.3min\n",
      "\n",
      "===== Mid 三模态 RF 最优超参 =====\n",
      "{'max_depth': 10, 'max_features': 0.8, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 291}\n",
      "CV 平均 R^2: 0.7992\n",
      "  -> OOF fold 1 / 10\n",
      "  -> OOF fold 2 / 10\n",
      "  -> OOF fold 3 / 10\n",
      "  -> OOF fold 4 / 10\n",
      "  -> OOF fold 5 / 10\n",
      "  -> OOF fold 6 / 10\n",
      "  -> OOF fold 7 / 10\n",
      "  -> OOF fold 8 / 10\n",
      "  -> OOF fold 9 / 10\n",
      "  -> OOF fold 10 / 10\n",
      "\n",
      "===== Mid 三模态 RF：train OOF 表现 =====\n",
      "r2: 0.8006\n",
      "mae: 0.3671\n",
      "rmse: 0.5253\n",
      "r: 0.8948\n",
      "\n",
      "===== Mid 三模态 RF 训练集表现 =====\n",
      "r2: 0.9423\n",
      "mae: 0.2059\n",
      "rmse: 0.2827\n",
      "r: 0.9718\n",
      "\n",
      "===== Mid 三模态 RF 测试集表现（独立 20%）=====\n",
      "r2: 0.7518\n",
      "mae: 0.3837\n",
      "rmse: 0.5523\n",
      "r: 0.8672\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8: RF 中期融合 (fused_emb + meta) =====\n",
    "\n",
    "# 1) RF 自己一套 8:2 划分（按 SMILES 分组，与 Cross-Attn 可以不同）\n",
    "gss_rf = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=2030)\n",
    "idx_all = np.arange(len(y_all))\n",
    "train_idx_rf, test_idx_rf = next(gss_rf.split(idx_all, y_all, groups))\n",
    "\n",
    "train_idx_rf = train_idx_rf.astype(int)\n",
    "test_idx_rf  = test_idx_rf.astype(int)\n",
    "\n",
    "print(\"RF 中期融合 train 样本数:\", len(train_idx_rf))\n",
    "print(\"RF 中期融合 test  样本数:\", len(test_idx_rf))\n",
    "\n",
    "X_fused_train = fused_all[train_idx_rf]\n",
    "X_fused_test  = fused_all[test_idx_rf]\n",
    "X_meta_train  = X_meta[train_idx_rf]\n",
    "X_meta_test   = X_meta[test_idx_rf]\n",
    "\n",
    "y_train_rf = y_all[train_idx_rf]\n",
    "y_test_rf  = y_all[test_idx_rf]\n",
    "groups_train_rf = groups[train_idx_rf]\n",
    "rowid_train_rf  = rowid[train_idx_rf]\n",
    "rowid_test_rf   = rowid[test_idx_rf]\n",
    "\n",
    "# 中期融合最终特征 = fused_emb + meta\n",
    "X_train_mid = np.concatenate([X_fused_train, X_meta_train], axis=1)\n",
    "X_test_mid  = np.concatenate([X_fused_test,  X_meta_test],  axis=1)\n",
    "\n",
    "print(\"X_train_mid 形状:\", X_train_mid.shape)\n",
    "print(\"X_test_mid  形状:\", X_test_mid.shape)\n",
    "\n",
    "# 2) RF 十折 GroupKFold + RandomizedSearchCV（和前面保持一致）\n",
    "param_distributions_mid = {\n",
    "    \"n_estimators\":      randint(200, 1001),\n",
    "    \"max_depth\":         [None, 10, 20, 30],\n",
    "    \"min_samples_split\": randint(2, 11),\n",
    "    \"min_samples_leaf\":  randint(1, 5),\n",
    "    \"max_features\":      [\"sqrt\", \"log2\", 0.3, 0.5, 0.8],\n",
    "}\n",
    "\n",
    "base_rf_mid = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=GLOBAL_SEED,\n",
    ")\n",
    "\n",
    "cv_rf = GroupKFold(n_splits=10)\n",
    "cv_indices_rf = list(cv_rf.split(X_train_mid, y_train_rf, groups_train_rf))\n",
    "\n",
    "rf_mid_search = RandomizedSearchCV(\n",
    "    estimator=base_rf_mid,\n",
    "    param_distributions=param_distributions_mid,\n",
    "    n_iter=30,\n",
    "    scoring=\"r2\",\n",
    "    cv=cv_indices_rf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=GLOBAL_SEED,\n",
    ")\n",
    "\n",
    "print(\"\\n==== [Mid T+G+P] RF 十折随机搜索 ====\")\n",
    "rf_mid_search.fit(X_train_mid, y_train_rf, groups=groups_train_rf)\n",
    "\n",
    "best_params_mid = rf_mid_search.best_params_\n",
    "best_cv_mid     = rf_mid_search.best_score_\n",
    "\n",
    "print(\"\\n===== Mid 三模态 RF 最优超参 =====\")\n",
    "print(best_params_mid)\n",
    "print(f\"CV 平均 R^2: {best_cv_mid:.4f}\")\n",
    "\n",
    "# 3) 用最优超参 + 同一套 folds，计算 train OOF 预测\n",
    "oof_pred_train_mid = np.zeros_like(y_train_rf, dtype=float)\n",
    "\n",
    "for fold_idx, (tr_idx, val_idx) in enumerate(cv_indices_rf, 1):\n",
    "    print(f\"  -> OOF fold {fold_idx} / {len(cv_indices_rf)}\")\n",
    "    rf_fold = RandomForestRegressor(\n",
    "        **best_params_mid,\n",
    "        n_jobs=-1,\n",
    "        random_state=GLOBAL_SEED + 200 + fold_idx,\n",
    "    )\n",
    "    rf_fold.fit(X_train_mid[tr_idx], y_train_rf[tr_idx])\n",
    "    oof_pred_train_mid[val_idx] = rf_fold.predict(X_train_mid[val_idx])\n",
    "\n",
    "metrics_oof_mid = compute_metrics(y_train_rf, oof_pred_train_mid)\n",
    "print(\"\\n===== Mid 三模态 RF：train OOF 表现 =====\")\n",
    "for k, v in metrics_oof_mid.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# 4) 在整个 train80% 上拟合最终 RF，并在 test20% 上评估\n",
    "best_rf_mid = RandomForestRegressor(\n",
    "    **best_params_mid,\n",
    "    n_jobs=-1,\n",
    "    random_state=GLOBAL_SEED + 300,\n",
    ")\n",
    "best_rf_mid.fit(X_train_mid, y_train_rf)\n",
    "\n",
    "y_train_mid_pred = best_rf_mid.predict(X_train_mid)\n",
    "y_test_mid_pred  = best_rf_mid.predict(X_test_mid)\n",
    "\n",
    "metrics_train_mid = compute_metrics(y_train_rf, y_train_mid_pred)\n",
    "metrics_test_mid  = compute_metrics(y_test_rf,  y_test_mid_pred)\n",
    "\n",
    "print(\"\\n===== Mid 三模态 RF 训练集表现 =====\")\n",
    "for k, v in metrics_train_mid.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== Mid 三模态 RF 测试集表现（独立 20%）=====\")\n",
    "for k, v in metrics_test_mid.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
