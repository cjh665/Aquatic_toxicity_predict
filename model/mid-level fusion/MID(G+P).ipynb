{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773f4e47-f119-468e-9f19-9e2303f8bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: 依赖 & 工具函数 =====\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ---- 全局随机种子 ----\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用设备:\", device)\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"计算 MAE / RMSE / R2 / Pearson_r\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2   = float(r2_score(y_true, y_pred))\n",
    "\n",
    "    if np.std(y_true) == 0 or np.std(y_pred) == 0:\n",
    "        pr = float(\"nan\")\n",
    "    else:\n",
    "        pr, _ = pearsonr(y_true, y_pred)\n",
    "        pr = float(pr)\n",
    "\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"Pearson_r\": pr}\n",
    "\n",
    "\n",
    "def np_encoder(o):\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    raise TypeError(f\"Type {type(o)} not serializable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523fafe-a768-4276-8809-7c07dc54e08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046eb4a0-c686-4013-9895-9b2b8640c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAPH_EMB_PATH: /root/Invertebrates_EC50_multi_fusion/graph/graph_outputs/reg_graph_embeddings.npy\n",
      "PHY_EMB_PATH  : /root/Invertebrates_EC50_multi_fusion/phychem/physchem_mlp_rf_v2/emb_physchem_mlp_all.npy\n",
      "DATA_PATH     : /root/fusion_dataset/with_physchem_excels/Invertebrates_EC50_unique_physchem.xlsx\n",
      "OUT_GP        : /root/Invertebrates_EC50_multi_fusion/mid(G+P)/graph_physchem_meta\n",
      "df 形状: (3620, 36)\n",
      "   row_id    SMILES_Canonical_RDKit  mgperL  mgperL_log\n",
      "0       0        [Cl-].[Cl-].[Zn+2]     1.3    0.113943\n",
      "1       1  O=S(=O)([O-])[O-].[Zn+2]     2.5    0.397940\n",
      "2       2        [Cl-].[Cl-].[Pb+2]    40.8    1.610660\n",
      "3       3  O=S(=O)([O-])[O-].[Cu+2]     1.9    0.278754\n",
      "4       4  O=S(=O)([O-])[O-].[Cu+2]     0.6   -0.221849\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: 路径 & 读取 df =====\n",
    "ROOT_MULTI = Path(\"/root/Invertebrates_EC50_multi_fusion\")\n",
    "\n",
    "# Graph 嵌入 + row_id\n",
    "GRAPH_DIR        = ROOT_MULTI / \"graph\" / \"graph_outputs\"\n",
    "GRAPH_EMB_PATH   = GRAPH_DIR / \"reg_graph_embeddings.npy\"\n",
    "GRAPH_ROWID_PATH = GRAPH_DIR / \"row_id_graph_for_emb.npy\"\n",
    "\n",
    "# PhysChem 嵌入 + row_id\n",
    "PHY_DIR        = ROOT_MULTI / \"phychem\" / \"physchem_mlp_rf_v2\"\n",
    "PHY_EMB_PATH   = PHY_DIR / \"emb_physchem_mlp_all.npy\"\n",
    "PHY_ROWID_PATH = PHY_DIR / \"row_id_clean.npy\"\n",
    "\n",
    "# 原始数据\n",
    "DATA_PATH = Path(\"/root/fusion_dataset/with_physchem_excels/Invertebrates_EC50_unique_physchem.xlsx\")\n",
    "\n",
    "# 输出目录\n",
    "MID_ROOT   = ROOT_MULTI / \"mid(G+P)\"\n",
    "OUT_GP     = MID_ROOT / \"graph_physchem_meta\"\n",
    "MODELS_CA  = OUT_GP / \"models_crossattn\"\n",
    "MODELS_RF  = OUT_GP / \"models_rf\"\n",
    "for d in [MID_ROOT, OUT_GP, MODELS_CA, MODELS_RF]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SMILES_COL   = \"SMILES_Canonical_RDKit\"\n",
    "DURATION_COL = \"Duration_Value(hour)\"\n",
    "EFFECT_COL   = \"Effect\"\n",
    "ENDPOINT_COL = \"Endpoint\"\n",
    "LABEL_RAW    = \"mgperL\"\n",
    "LABEL_LOG    = \"mgperL_log\"\n",
    "LABEL_COL    = LABEL_LOG\n",
    "\n",
    "print(\"GRAPH_EMB_PATH:\", GRAPH_EMB_PATH)\n",
    "print(\"PHY_EMB_PATH  :\", PHY_EMB_PATH)\n",
    "print(\"DATA_PATH     :\", DATA_PATH)\n",
    "print(\"OUT_GP        :\", OUT_GP)\n",
    "\n",
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "if \"row_id\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={\"index\": \"row_id\"})\n",
    "df[\"row_id\"] = df[\"row_id\"].astype(int)\n",
    "\n",
    "if LABEL_LOG not in df.columns:\n",
    "    df[LABEL_RAW] = pd.to_numeric(df[LABEL_RAW], errors=\"coerce\")\n",
    "    mask_valid = df[LABEL_RAW] > 0\n",
    "    df[LABEL_LOG] = np.where(mask_valid, np.log10(df[LABEL_RAW]), np.nan)\n",
    "\n",
    "print(\"df 形状:\", df.shape)\n",
    "print(df[[\"row_id\", SMILES_COL, LABEL_RAW, LABEL_LOG]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5262a-0c47-4df5-b13b-a97ef0bb9d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565abba5-4d9b-43e8-aa42-3027ab9650f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_emb 形状: (3213, 256)\n",
      "rowid_graph 范围: 1 → 3619\n",
      "phys_emb 形状: (3406, 64)\n",
      "rowid_phys 范围: 0 → 3619\n",
      "Graph+Phys row_id 交集初始样本数: 3103\n",
      "过滤后样本数: 3103\n",
      "X_graph 形状: (3103, 256)\n",
      "X_phys  形状: (3103, 64)\n",
      "dur_raw 形状: (3103, 1)\n",
      "cat_all 形状: (3103, 3)\n",
      "总样本数 N: 3103\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: 加载 Graph & PhysChem，对齐 + 构造 meta =====\n",
    "\n",
    "graph_emb   = np.load(GRAPH_EMB_PATH)\n",
    "rowid_graph = np.load(GRAPH_ROWID_PATH).astype(int)\n",
    "print(\"graph_emb 形状:\", graph_emb.shape)\n",
    "print(\"rowid_graph 范围:\", rowid_graph.min(), \"→\", rowid_graph.max())\n",
    "\n",
    "phys_emb   = np.load(PHY_EMB_PATH)\n",
    "rowid_phys = np.load(PHY_ROWID_PATH).astype(int)\n",
    "print(\"phys_emb 形状:\", phys_emb.shape)\n",
    "print(\"rowid_phys 范围:\", rowid_phys.min(), \"→\", rowid_phys.max())\n",
    "\n",
    "df_indexed = df.set_index(\"row_id\")\n",
    "\n",
    "ids_graph = set(rowid_graph.tolist())\n",
    "ids_phys  = set(rowid_phys.tolist())\n",
    "ids_all   = set(df_indexed.index.tolist())\n",
    "\n",
    "ids_inter = sorted(list(ids_graph & ids_phys & ids_all))\n",
    "print(\"Graph+Phys row_id 交集初始样本数:\", len(ids_inter))\n",
    "\n",
    "idx_map_graph = {rid: i for i, rid in enumerate(rowid_graph)}\n",
    "idx_map_phys  = {rid: i for i, rid in enumerate(rowid_phys)}\n",
    "\n",
    "meta_list    = []\n",
    "X_graph_list = []\n",
    "X_phys_list  = []\n",
    "y_list       = []\n",
    "rid_list     = []\n",
    "\n",
    "for rid in ids_inter:\n",
    "    row_meta = df_indexed.loc[rid]\n",
    "    label = row_meta[LABEL_COL]\n",
    "    if pd.isna(label) or not np.isfinite(label):\n",
    "        continue\n",
    "    if pd.isna(row_meta.get(DURATION_COL)) or pd.isna(row_meta.get(EFFECT_COL)) or pd.isna(row_meta.get(ENDPOINT_COL)):\n",
    "        continue\n",
    "\n",
    "    meta_list.append(row_meta)\n",
    "    X_graph_list.append(graph_emb[idx_map_graph[rid]])\n",
    "    X_phys_list.append(phys_emb[idx_map_phys[rid]])\n",
    "    y_list.append(label)\n",
    "    rid_list.append(rid)\n",
    "\n",
    "meta_gp  = pd.DataFrame(meta_list).reset_index(drop=True)\n",
    "X_graph  = np.stack(X_graph_list, axis=0)\n",
    "X_phys   = np.stack(X_phys_list, axis=0)\n",
    "y_all    = np.array(y_list, dtype=float)\n",
    "rowid_all= np.array(rid_list, dtype=int)\n",
    "\n",
    "print(\"过滤后样本数:\", len(y_all))\n",
    "print(\"X_graph 形状:\", X_graph.shape)\n",
    "print(\"X_phys  形状:\", X_phys.shape)\n",
    "\n",
    "# Duration\n",
    "meta_gp[DURATION_COL] = pd.to_numeric(meta_gp[DURATION_COL], errors=\"coerce\")\n",
    "dur_median = meta_gp[DURATION_COL].median()\n",
    "meta_gp[DURATION_COL] = meta_gp[DURATION_COL].fillna(dur_median)\n",
    "dur_raw = meta_gp[[DURATION_COL]].values.astype(float)\n",
    "\n",
    "# One-hot: Effect + Endpoint\n",
    "cat_cols = [EFFECT_COL, ENDPOINT_COL]\n",
    "cat_dummies = pd.get_dummies(meta_gp[cat_cols], dummy_na=False)\n",
    "cat_all = cat_dummies.values.astype(float)\n",
    "cat_feature_names = list(cat_dummies.columns)\n",
    "\n",
    "print(\"dur_raw 形状:\", dur_raw.shape)\n",
    "print(\"cat_all 形状:\", cat_all.shape)\n",
    "\n",
    "groups_all = meta_gp[SMILES_COL].astype(str).values\n",
    "print(\"总样本数 N:\", len(y_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd65126-70ff-4210-8ae5-ec3194b830d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87581e54-831e-44c9-8257-a225966675ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_graph_raw 形状: (3103, 256)\n",
      "X_phys_raw  形状: (3103, 64)\n",
      "dur_raw     形状: (3103, 1)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: 原始数值块 =====\n",
    "X_graph_raw = X_graph\n",
    "X_phys_raw  = X_phys\n",
    "\n",
    "print(\"X_graph_raw 形状:\", X_graph_raw.shape)\n",
    "print(\"X_phys_raw  形状:\", X_phys_raw.shape)\n",
    "print(\"dur_raw     形状:\", dur_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876233d-f337-42a7-bd0f-52848b52fd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c8581e-5ffd-431f-8af9-e081b698d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5: Dataset & Cross-Attn 模型（Graph+Phys） =====\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, X1, X2, y):\n",
    "        self.X1 = torch.from_numpy(X1).float()\n",
    "        self.X2 = torch.from_numpy(X2).float()\n",
    "        self.y  = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class CrossAttnEncoder(nn.Module):\n",
    "    def __init__(self, dim_a, dim_b, hidden_dim=256, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj_a = nn.Linear(dim_a, hidden_dim)\n",
    "        self.proj_b = nn.Linear(dim_b, hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, xa, xb):\n",
    "        h_a = self.proj_a(xa)\n",
    "        h_b = self.proj_b(xb)\n",
    "        tokens = torch.stack([h_a, h_b], dim=0)\n",
    "        attn_out, _ = self.attn(tokens, tokens, tokens)\n",
    "        fused = attn_out.mean(dim=0)\n",
    "        return self.dropout(fused)\n",
    "\n",
    "\n",
    "class CrossAttnWithHead(nn.Module):\n",
    "    def __init__(self, dim_a, dim_b,\n",
    "                 hidden_dim=256,\n",
    "                 num_heads=4,\n",
    "                 mlp_hidden=512,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = CrossAttnEncoder(dim_a, dim_b, hidden_dim, num_heads, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, xa, xb):\n",
    "        fused = self.encoder(xa, xb)\n",
    "        out   = self.head(fused).squeeze(-1)\n",
    "        return out, fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b43152-c670-4897-a528-33691638f3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bc1a94-5645-473c-966d-283641c6591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Attn G+P train 样本数: 2510\n",
      "Cross-Attn G+P val   样本数: 593\n",
      "[Cross-Attn G+P] Epoch 001 | train MAE = 0.7107, val MAE = 0.5925\n",
      "[Cross-Attn G+P] Epoch 002 | train MAE = 0.5778, val MAE = 0.5691\n",
      "[Cross-Attn G+P] Epoch 003 | train MAE = 0.5620, val MAE = 0.5692\n",
      "[Cross-Attn G+P] Epoch 004 | train MAE = 0.5517, val MAE = 0.5788\n",
      "[Cross-Attn G+P] Epoch 005 | train MAE = 0.5454, val MAE = 0.5649\n",
      "[Cross-Attn G+P] Epoch 006 | train MAE = 0.5452, val MAE = 0.5915\n",
      "[Cross-Attn G+P] Epoch 007 | train MAE = 0.5342, val MAE = 0.5820\n",
      "[Cross-Attn G+P] Epoch 008 | train MAE = 0.5224, val MAE = 0.5734\n",
      "[Cross-Attn G+P] Epoch 009 | train MAE = 0.5155, val MAE = 0.5752\n",
      "[Cross-Attn G+P] Epoch 010 | train MAE = 0.5152, val MAE = 0.5699\n",
      "[Cross-Attn G+P] Epoch 011 | train MAE = 0.5068, val MAE = 0.5729\n",
      "[Cross-Attn G+P] Epoch 012 | train MAE = 0.5091, val MAE = 0.5610\n",
      "[Cross-Attn G+P] Epoch 013 | train MAE = 0.5010, val MAE = 0.5785\n",
      "[Cross-Attn G+P] Epoch 014 | train MAE = 0.5013, val MAE = 0.5529\n",
      "[Cross-Attn G+P] Epoch 015 | train MAE = 0.4942, val MAE = 0.5587\n",
      "[Cross-Attn G+P] Epoch 016 | train MAE = 0.4974, val MAE = 0.5730\n",
      "[Cross-Attn G+P] Epoch 017 | train MAE = 0.4874, val MAE = 0.5536\n",
      "[Cross-Attn G+P] Epoch 018 | train MAE = 0.4799, val MAE = 0.5720\n",
      "[Cross-Attn G+P] Epoch 019 | train MAE = 0.4839, val MAE = 0.5820\n",
      "[Cross-Attn G+P] Epoch 020 | train MAE = 0.4806, val MAE = 0.5812\n",
      "[Cross-Attn G+P] Epoch 021 | train MAE = 0.4701, val MAE = 0.5654\n",
      "[Cross-Attn G+P] Epoch 022 | train MAE = 0.4697, val MAE = 0.5949\n",
      "[Cross-Attn G+P] Epoch 023 | train MAE = 0.4667, val MAE = 0.5916\n",
      "[Cross-Attn G+P] Epoch 024 | train MAE = 0.4660, val MAE = 0.5744\n",
      "[Cross-Attn G+P] Early stopping at epoch 24, best_epoch = 14\n",
      "fused_all_G_P 形状: (3103, 256)\n",
      "\n",
      "✅ Cross-Attn (Graph+Phys) 训练完成，fused_all_G_P 已保存。\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: Cross-Attn 按 SMILES 8:2 划分 & 训练（Graph+Phys） =====\n",
    "\n",
    "gss_ca = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=4025)\n",
    "N = len(y_all)\n",
    "idx_all = np.arange(N)\n",
    "\n",
    "ca_train_idx, ca_val_idx = next(\n",
    "    gss_ca.split(np.zeros(N), y_all, groups_all)\n",
    ")\n",
    "ca_train_idx = np.array(ca_train_idx, dtype=np.int64)\n",
    "ca_val_idx   = np.array(ca_val_idx, dtype=np.int64)\n",
    "\n",
    "print(\"Cross-Attn G+P train 样本数:\", len(ca_train_idx))\n",
    "print(\"Cross-Attn G+P val   样本数:\", len(ca_val_idx))\n",
    "\n",
    "np.save(OUT_GP / \"ca_train_idx_G_P.npy\", ca_train_idx)\n",
    "np.save(OUT_GP / \"ca_val_idx_G_P.npy\",   ca_val_idx)\n",
    "\n",
    "# 标准化（仅用 Cross-Attn train80% 拟合）\n",
    "scaler_graph_ca = StandardScaler().fit(X_graph_raw[ca_train_idx])\n",
    "scaler_phys_ca  = StandardScaler().fit(X_phys_raw[ca_train_idx])\n",
    "\n",
    "X_graph_ca_all_std = scaler_graph_ca.transform(X_graph_raw)\n",
    "X_phys_ca_all_std  = scaler_phys_ca.transform(X_phys_raw)\n",
    "\n",
    "X_graph_ca_train = X_graph_ca_all_std[ca_train_idx]\n",
    "X_phys_ca_train  = X_phys_ca_all_std[ca_train_idx]\n",
    "y_ca_train       = y_all[ca_train_idx]\n",
    "\n",
    "X_graph_ca_val = X_graph_ca_all_std[ca_val_idx]\n",
    "X_phys_ca_val  = X_phys_ca_all_std[ca_val_idx]\n",
    "y_ca_val       = y_all[ca_val_idx]\n",
    "\n",
    "batch_size = 64\n",
    "ds_ca_tr  = PairDataset(X_graph_ca_train, X_phys_ca_train, y_ca_train)\n",
    "ds_ca_val = PairDataset(X_graph_ca_val,   X_phys_ca_val,   y_ca_val)\n",
    "\n",
    "dl_ca_tr  = DataLoader(ds_ca_tr,  batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "dl_ca_val = DataLoader(ds_ca_val, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "dim_a = X_graph_ca_train.shape[1]\n",
    "dim_b = X_phys_ca_train.shape[1]\n",
    "\n",
    "model_ca = CrossAttnWithHead(\n",
    "    dim_a=dim_a,\n",
    "    dim_b=dim_b,\n",
    "    hidden_dim=256,\n",
    "    num_heads=4,\n",
    "    mlp_hidden=512,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_ca.parameters(),\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "max_epochs = 80\n",
    "patience   = 10\n",
    "best_val_mae = float(\"inf\")\n",
    "best_state_dict = None\n",
    "best_epoch = -1\n",
    "epochs_no_improve = 0\n",
    "\n",
    "history_ca = {\"train_mae\": [], \"val_mae\": []}\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    model_ca.train()\n",
    "    train_abs_err = []\n",
    "\n",
    "    for X1_b, X2_b, y_b in dl_ca_tr:\n",
    "        X1_b = X1_b.to(device)\n",
    "        X2_b = X2_b.to(device)\n",
    "        y_b  = y_b.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, fused = model_ca(X1_b, X2_b)\n",
    "        loss = loss_fn(y_hat, y_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_ca.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_abs_err.append(torch.abs(y_hat.detach() - y_b).cpu().numpy())\n",
    "\n",
    "    train_mae = float(np.mean(np.concatenate(train_abs_err)))\n",
    "    history_ca[\"train_mae\"].append(train_mae)\n",
    "\n",
    "    model_ca.eval()\n",
    "    val_abs_err = []\n",
    "    with torch.no_grad():\n",
    "        for X1_b, X2_b, y_b in dl_ca_val:\n",
    "            X1_b = X1_b.to(device)\n",
    "            X2_b = X2_b.to(device)\n",
    "            y_b  = y_b.to(device)\n",
    "\n",
    "            y_hat, fused = model_ca(X1_b, X2_b)\n",
    "            val_abs_err.append(torch.abs(y_hat - y_b).cpu().numpy())\n",
    "\n",
    "    val_mae = float(np.mean(np.concatenate(val_abs_err)))\n",
    "    history_ca[\"val_mae\"].append(val_mae)\n",
    "\n",
    "    print(f\"[Cross-Attn G+P] Epoch {epoch:03d} | train MAE = {train_mae:.4f}, val MAE = {val_mae:.4f}\")\n",
    "\n",
    "    if val_mae < best_val_mae - 1e-4:\n",
    "        best_val_mae = val_mae\n",
    "        best_state_dict = {k: v.cpu().clone() for k, v in model_ca.state_dict().items()}\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"[Cross-Attn G+P] Early stopping at epoch {epoch}, best_epoch = {best_epoch}\")\n",
    "            break\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    model_ca.load_state_dict(best_state_dict)\n",
    "model_ca.to(device)\n",
    "model_ca.eval()\n",
    "\n",
    "encoder_gp = model_ca.encoder\n",
    "encoder_gp.eval().to(device)\n",
    "\n",
    "ds_all_ca = PairDataset(X_graph_ca_all_std, X_phys_ca_all_std, y_all)\n",
    "dl_all_ca = DataLoader(ds_all_ca, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "fused_all_list = []\n",
    "with torch.no_grad():\n",
    "    for X1_b, X2_b, y_b in dl_all_ca:\n",
    "        X1_b = X1_b.to(device)\n",
    "        X2_b = X2_b.to(device)\n",
    "        fused = encoder_gp(X1_b, X2_b)\n",
    "        fused_all_list.append(fused.cpu().numpy())\n",
    "\n",
    "fused_all_GP = np.concatenate(fused_all_list, axis=0)\n",
    "print(\"fused_all_G_P 形状:\", fused_all_GP.shape)\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": best_state_dict,\n",
    "        \"config\": {\n",
    "            \"dim_a\": dim_a,\n",
    "            \"dim_b\": dim_b,\n",
    "            \"hidden_dim\": 256,\n",
    "            \"num_heads\": 4,\n",
    "            \"mlp_hidden\": 512,\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    MODELS_CA / \"crossattn_G_P_best.pt\"\n",
    ")\n",
    "\n",
    "np.save(OUT_GP / \"fused_all_G_P.npy\", fused_all_GP)\n",
    "np.save(OUT_GP / \"row_id_all_G_P.npy\", rowid_all)\n",
    "np.save(OUT_GP / \"y_all_G_P.npy\",      y_all)\n",
    "np.save(OUT_GP / \"groups_all_G_P.npy\", groups_all)\n",
    "\n",
    "with open(OUT_GP / \"crossattn_G_P_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_val_mae\": float(best_val_mae),\n",
    "            \"history\": history_ca,\n",
    "            \"n_all\": int(len(y_all)),\n",
    "            \"n_train\": int(len(ca_train_idx)),\n",
    "            \"n_val\": int(len(ca_val_idx)),\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "        default=np_encoder,\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Cross-Attn (Graph+Phys) 训练完成，fused_all_G_P 已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06679b7c-5256-4cc6-bc87-c7591027561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c820f02-3315-4346-a25f-519ad09f2766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF G+P train 样本数: 2462\n",
      "RF G+P test  样本数: 641\n",
      "dur_all_std_rf 形状: (3103, 1)\n",
      "X_all_RF 形状: (3103, 260)\n",
      "X_train_RF 形状: (2462, 260)\n",
      "X_test_RF  形状: (641, 260)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: RF 端 8:2 划分 & 构造 RF 输入特征 (G+P) =====\n",
    "\n",
    "gss_rf = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=4031)\n",
    "N = len(y_all)\n",
    "\n",
    "rf_train_idx, rf_test_idx = next(\n",
    "    gss_rf.split(np.zeros(N), y_all, groups_all)\n",
    ")\n",
    "rf_train_idx = np.array(rf_train_idx, dtype=np.int64)\n",
    "rf_test_idx  = np.array(rf_test_idx, dtype=np.int64)\n",
    "\n",
    "print(\"RF G+P train 样本数:\", len(rf_train_idx))\n",
    "print(\"RF G+P test  样本数:\", len(rf_test_idx))\n",
    "\n",
    "np.save(OUT_GP / \"rf_train_idx_G_P.npy\", rf_train_idx)\n",
    "np.save(OUT_GP / \"rf_test_idx_G_P.npy\",  rf_test_idx)\n",
    "\n",
    "scaler_dur_rf = StandardScaler().fit(dur_raw[rf_train_idx])\n",
    "dur_all_std_rf = scaler_dur_rf.transform(dur_raw)\n",
    "print(\"dur_all_std_rf 形状:\", dur_all_std_rf.shape)\n",
    "\n",
    "X_all_RF = np.concatenate(\n",
    "    [fused_all_GP, dur_all_std_rf, cat_all],\n",
    "    axis=1\n",
    ")\n",
    "print(\"X_all_RF 形状:\", X_all_RF.shape)\n",
    "\n",
    "X_train_RF = X_all_RF[rf_train_idx]\n",
    "y_train_RF = y_all[rf_train_idx]\n",
    "groups_train_RF = groups_all[rf_train_idx]\n",
    "\n",
    "X_test_RF  = X_all_RF[rf_test_idx]\n",
    "y_test_RF  = y_all[rf_test_idx]\n",
    "\n",
    "print(\"X_train_RF 形状:\", X_train_RF.shape)\n",
    "print(\"X_test_RF  形状:\", X_test_RF.shape)\n",
    "\n",
    "np.save(OUT_GP / \"X_all_RF_G_P.npy\", X_all_RF)\n",
    "np.save(OUT_GP / \"X_train_RF_G_P.npy\", X_train_RF)\n",
    "np.save(OUT_GP / \"X_test_RF_G_P.npy\",  X_test_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef0a40d-e237-4227-b128-a7c4e2829d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5053c75e-4abb-4676-b97f-4ccf119ea193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始在 RF 80% train 上做十折 GroupKFold 随机超参搜索（G+P fused + meta）...\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=20.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.1min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 9.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  57.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=18.4min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  45.3s\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 7.1min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 3.1min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=20.4min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=10.1min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.1min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=10.3min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 5.7min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.3min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 3.5min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=24.6min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.0min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 8.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=18.9min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  43.5s\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 7.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 2.9min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=20.4min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=11.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.2min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 9.7min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 5.5min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.3min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 3.6min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=20.0min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=20.0min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=17.8min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  42.0s\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 6.9min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 3.2min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=21.7min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=10.2min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=18.7min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time= 3.4min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 3.6min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=27.5min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 8.9min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=18.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time= 3.0min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=12.7min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time= 8.8min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 5.6min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 5.0min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 6.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 2.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=17.4min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.9min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 3.5min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=20.6min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 2.9min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.0min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 8.6min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 9.1min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time= 2.6min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time= 2.6min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time= 2.9min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  35.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  41.4s\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=12.9min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time= 8.6min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 5.0min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 5.8min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=10.0min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=17.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time= 3.5min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 4.0min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time=20.3min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.6min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 8.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time=19.2min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 6.8min\n",
      "[CV] END max_depth=None, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=14.0min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=800; total time= 8.7min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 4.8min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 5.1min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 2.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=18.2min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 4.6min\n",
      "[CV] END max_depth=10, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 3.3min\n",
      "\n",
      "[G+P+meta→RF] 最优超参：\n",
      "{'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.3, 'max_depth': 30}\n",
      "最优 CV 分数 (neg MAE): -0.4923755135756435\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=27.2min\n",
      "[CV] END max_depth=40, max_features=0.3, min_samples_leaf=2, min_samples_split=10, n_estimators=800; total time= 8.6min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 8.7min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.7min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=800; total time= 2.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time= 3.1min\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 7.4min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 3.0min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=21.3min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=1, min_samples_split=2, n_estimators=800; total time=10.2min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=18.8min\n",
      "[CV] END max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 8.3min\n",
      "\n",
      "===== [G+P fused + meta → RF] RF train80% 指标 =====\n",
      "MAE: 0.2273\n",
      "RMSE: 0.3391\n",
      "R2: 0.9131\n",
      "Pearson_r: 0.9594\n",
      "\n",
      "===== [G+P fused + meta → RF] RF test20% 指标 =====\n",
      "MAE: 0.4997\n",
      "RMSE: 0.6795\n",
      "R2: 0.6835\n",
      "Pearson_r: 0.8282\n",
      "[CV] END max_depth=40, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=26.1min\n",
      "[CV] END max_depth=None, max_features=0.5, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 4.2min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=30, max_features=0.5, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 9.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  47.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  45.8s\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=1000; total time= 3.0min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  39.6s\n",
      "[CV] END max_depth=10, max_features=0.5, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 7.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 3.1min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=1000; total time= 2.8min\n",
      "[CV] END max_depth=20, max_features=0.8, min_samples_leaf=1, min_samples_split=10, n_estimators=800; total time=21.0min\n",
      "[CV] END max_depth=30, max_features=0.3, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 5.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 3.3min\n",
      "[CV] END max_depth=10, max_features=0.8, min_samples_leaf=1, min_samples_split=5, n_estimators=800; total time=18.4min\n",
      "[CV] END max_depth=None, max_features=0.3, min_samples_leaf=2, min_samples_split=2, n_estimators=1000; total time= 8.3min\n",
      "\n",
      "✅ RF (G+P fused + meta) 训练 & 评估完成。\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8: RF 十折 CV + RandomizedSearchCV (G+P) =====\n",
    "\n",
    "param_distributions_rf = {\n",
    "    \"n_estimators\":      [200, 300, 500, 800, 1000],\n",
    "    \"max_depth\":         [None, 10, 20, 30, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\":  [1, 2, 4],\n",
    "    \"max_features\":      [\"sqrt\", \"log2\", 0.3, 0.5, 0.8],\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    random_state=GLOBAL_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "cv_inner = GroupKFold(n_splits=10)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions_rf,\n",
    "    n_iter=30,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=cv_inner,\n",
    "    n_jobs=-1,\n",
    "    random_state=GLOBAL_SEED,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"开始在 RF 80% train 上做十折 GroupKFold 随机超参搜索（G+P fused + meta）...\")\n",
    "rf_search.fit(X_train_RF, y_train_RF, groups=groups_train_RF)\n",
    "\n",
    "best_params_rf = rf_search.best_params_\n",
    "best_score_rf  = rf_search.best_score_\n",
    "\n",
    "print(\"\\n[G+P+meta→RF] 最优超参：\")\n",
    "print(best_params_rf)\n",
    "print(\"最优 CV 分数 (neg MAE):\", best_score_rf)\n",
    "\n",
    "rf_final = RandomForestRegressor(\n",
    "    **best_params_rf,\n",
    "    random_state=GLOBAL_SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_final.fit(X_train_RF, y_train_RF)\n",
    "\n",
    "y_train_pred_RF = rf_final.predict(X_train_RF)\n",
    "y_test_pred_RF  = rf_final.predict(X_test_RF)\n",
    "\n",
    "metrics_train_RF = compute_regression_metrics(y_train_RF, y_train_pred_RF)\n",
    "metrics_test_RF  = compute_regression_metrics(y_test_RF,  y_test_pred_RF)\n",
    "\n",
    "print(\"\\n===== [G+P fused + meta → RF] RF train80% 指标 =====\")\n",
    "for k, v in metrics_train_RF.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n===== [G+P fused + meta → RF] RF test20% 指标 =====\")\n",
    "for k, v in metrics_test_RF.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": rf_final,\n",
    "        \"scaler_graph_ca\": scaler_graph_ca,\n",
    "        \"scaler_phys_ca\": scaler_phys_ca,\n",
    "        \"encoder_state_dict\": best_state_dict,\n",
    "        \"scaler_dur_rf\": scaler_dur_rf,\n",
    "        \"cat_feature_names\": cat_feature_names,\n",
    "        \"config\": {\n",
    "            \"hidden_dim\": 256,\n",
    "            \"GLOBAL_SEED\": int(GLOBAL_SEED),\n",
    "            \"param_distributions_rf\": param_distributions_rf,\n",
    "        },\n",
    "    },\n",
    "    MODELS_RF / \"rf_G_P_meta_from_CA.joblib\"\n",
    ")\n",
    "\n",
    "np.save(OUT_GP / \"y_train_RF_G_P.npy\", y_train_RF)\n",
    "np.save(OUT_GP / \"y_test_RF_G_P.npy\",  y_test_RF)\n",
    "np.save(OUT_GP / \"y_train_pred_RF_G_P.npy\", y_train_pred_RF)\n",
    "np.save(OUT_GP / \"y_test_pred_RF_G_P.npy\",  y_test_pred_RF)\n",
    "\n",
    "with open(OUT_GP / \"metrics_RF_G_P_meta_from_CA.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_params_rf\": best_params_rf,\n",
    "            \"best_score_cv_neg_mae\": float(best_score_rf),\n",
    "            \"train80_metrics\": metrics_train_RF,\n",
    "            \"test20_metrics\": metrics_test_RF,\n",
    "            \"n_all\": int(len(y_all)),\n",
    "            \"n_train80_rf\": int(len(y_train_RF)),\n",
    "            \"n_test20_rf\": int(len(y_test_RF)),\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "        default=np_encoder,\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ RF (G+P fused + meta) 训练 & 评估完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df27cff-c5d3-45a0-be32-cbdd9858d512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
